[["index.html", "Case Studies Preface", " Case Studies 2021-04-08 Preface This book is the result of a student projects for Case Studies course at the Warsaw University of Technology. Each team prepared an article on one of the topics selected from reproducibility, imputation, and interpretability. This project is inspired by a book Limitations of Interpretable Machine Learning Methods created at the Department of Statistics, LMU Munich XAI Stories. Case studies for eXplainable Artificial Intelligence done at the Warsaw University of Technology and at the University of Warsaw and ML Case Studies during a case study a year ago. We used the LIML project as the cornerstone for this repository. The cover created by Anna Kozak. Creative Commons License This book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["technical-setup.html", "Technical Setup", " Technical Setup The book chapters are written in the Markdown language. The simulations, data examples and visualizations were created with R (R Core Team 2018) and Python. The book was compiled with the bookdown package. We collaborated using git and github. For details, head over to the book’s repository. "],["explainable-artificial-intelligence.html", "Chapter 1 Explainable Artificial Intelligence", " Chapter 1 Explainable Artificial Intelligence Author: Anna Kozak Machine Learning is used more and more in virtually any aspect of our life. We train models to predict the future in banking, telecommunication, insurance, industry, and many other areas. The models give us predictions, however, very often we do not know how they are calculated. Can we trust these predictions? Why should we use the results of models which we do not fully understand? This results in a lack of understanding of the results obtained, so there is now a strong need to explain the decisions made by the non-interpretable models called black boxes. There are several tools for exploring and explaining the predictive models, which allow to understanding how they are works. During the class, we explored methods of explaining global as well as local, which you can read more about in the Explanatory Model Analysis (Biecek and Burzykowski 2021) book. Teams work on data from a Kaggle that described problems in the world around us. Each team was responsible for analyzing, modeling, and building explanations for complex models. Each chapter includes a story about how to use explainable AI to understand the model. "],["explainable-artificial-inteligence-r.html", "Chapter 2 Explainable artificial inteligence (R)", " Chapter 2 Explainable artificial inteligence (R) "],["deep-learning-1.html", "Chapter 3 Deep Learning 1", " Chapter 3 Deep Learning 1 "],["deep-learning-2.html", "Chapter 4 Deep Learning 2", " Chapter 4 Deep Learning 2 "],["machine-learning.html", "Chapter 5 Machine Learning", " Chapter 5 Machine Learning Author: Hubert Baniecki An ever-growing domain of machine learning decision systems in medicine has crossed ways with the COVID-19 pandemic. Precariously, a vast majority of the proposed predictive models focus on achieving high performance; while overlooking comprehensive validation. Nowadays, providing representative data, model explainability, even bias detection become mandatory for responsible prediction making in high-stakes medical applications. The following chapters introduce new views into the already published work on the topic of patients’ COVID-19 mortality prognosis using supervised machine learning: 1. TBA 2. TBA 3. TBA 4. TBA "],["rashomonml.html", "Chapter 6 RashomonML", " Chapter 6 RashomonML Biecek, Przemyslaw, and Tomasz Burzykowski. 2021. Explanatory Model Analysis. Chapman; Hall/CRC, New York. https://pbiecek.github.io/ema/. R Core Team. 2018. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. "]]
