[["index.html", "Case Studies Preface", " Case Studies Faculty of Mathematics and Information Science, Warsaw University of Technology 2021-05-23 Preface This book is the result of a student projects for Case Studies course at the Warsaw University of Technology. Each team prepared an article on one of the topics selected from reproducibility, imputation, and interpretability. This project is inspired by a book Limitations of Interpretable Machine Learning Methods created at the Department of Statistics, LMU Munich XAI Stories. Case studies for eXplainable Artificial Intelligence done at the Warsaw University of Technology and at the University of Warsaw, and ML Case Studies during the last year’s course. We used the LIML project as the cornerstone for this repository. The cover created by Anna Kozak. Creative Commons License This book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["technical-setup.html", "Technical Setup", " Technical Setup The book chapters are written in the Markdown language. The simulations, data examples and visualizations were created with R (R Core Team 2018) and Python. The book was compiled with the bookdown package. We collaborated using git and github. For details, head over to the book’s repository. References R Core Team. (2018). R: A language and environment for statistical computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/ "],["explainable-artificial-intelligence-1.html", "Chapter 1 Explainable Artificial Intelligence 1", " Chapter 1 Explainable Artificial Intelligence 1 Author: Anna Kozak Machine Learning is used more and more in virtually any aspect of our life. We train models to predict the future in banking, telecommunication, insurance, industry, and many other areas. The models give us predictions, however, very often we do not know how they are calculated. Can we trust these predictions? Why should we use the results of models which we do not fully understand? This results in a lack of understanding of the results obtained, so there is now a strong need to explain the decisions made by the non-interpretable models called black boxes. There are several tools for exploring and explaining the predictive models, which allow to understanding how they are works. During the class, we explored methods of explaining global as well as local, which you can read more about in the Explanatory Model Analysis (Biecek and Burzykowski 2021) book. Teams work on data from a Kaggle that described problems in the world around us. Each team was responsible for analyzing, modeling, and building explanations for complex models. Each chapter includes a story about how to use explainable AI to understand the model. References Biecek, P., &amp; Burzykowski, T. (2021). Explanatory Model Analysis. Chapman; Hall/CRC, New York. https://pbiecek.github.io/ema/ "],["xai1-explainable-cards.html", "1.1 Explaining Credit Card Customers churns", " 1.1 Explaining Credit Card Customers churns Authors: Katarzyna Solawa, Przemysław Chojecki, Bartosz Sawicki (Warsaw University of Technology) 1.1.1 Global explanations In this section we describe our discoveries made by using explenatory methods globally. 1.1.1.1 Permutational Feature Importance We calculated permutational feature importance for XGBoost model, Random Forest model and a group of logistic regression models. The regression models were created with L1 regularization and different C coefficient was applied among the group. This coefficient is an inverse of a penalty term in L1 regularization, which means the smaller it is, the more weights shrinkage we expect. We examined if such shrinkage is noticeable in Permutational Feature Importance method. Then, we compared PFI obtained from different models. 1.1.1.1.1 Logistic Regression Models Figure 1.1: Permutational Feature Impotance of Total_Ct_Chng_Q4_Q1 for the group of logistic regression models The variable with the highest (among the group of logistic regressions) drop-out loss is shown in 1.1. The drop-out increases with the increase of C coefficient. The feature is more important for models with low regularization parameter, therefore it was shrinked by the Lasso. Figure 1.2: Permutational Feature Impotance of Total_Revolving_Bal for the group of logistic regression models 1.2 presents a variable importance plot of Total_Revolving_Bal feature, which has the second highest drop-out loss. It was not regularized, because drop-out loss decreases with the increase of C. It is the only column, which has this property. Figure 1.3: Permutational Feature Impotance of Gender for the group of logistic regression models Figure 1.4: Permutational Feature Impotance of Avg_Utilization_Ratio for the group of logistic regression models On the other plots, such as 1.3 and 1.4, the shrinkage made by L1 regularization is clearly visable. Models with high regularization parameter, and accordingly low C parameter, have smaller drop-out losses, which indicates lower importance of features. Drop-out loss increases proportionally to C parameter in nearly all of 21 columns. This shows that the effects of Lasso regularization can be observed in variable importance plots of logistic regression models. 1.1.1.1.2 XGBoost and Random Forest models In 1.5 we can observe that the most important column for both models is Total_Trans_Amt. This outcome can be logically explained: customers who do not use their credit card to execute many valuable transactions probably do not need that service, consequently resign. However, the drop-out loss for that column for the XGBoost is over 2 times higher than for the Random Forest, which means that the prior model bases its prediction on this column more than the latter model. Furthermore, more features are important for the XGBoost than for the Random Forest. We suppose this is a result of the models different training processes. New iterations (trees) in XGB are based on observations that were previously predicted incorrectly, thus new columns are taken into consideration to represent the differences between the observations. On the other hand, the Random Forest model selects the subset of the features randomly in each tree. Figure 1.5: Top 9 most important features in XGBoost and RandomForest feature importance comparison 1.1.1.1.3 Models comparison We compared the permutational feature importance of the group of logistic regression models, XGBoost model and Random Forest model. We can see in 1.6 the drop-out loss in XGBoost is similar to drop-out in logistic regression models. Figure 1.6: Permutational Feature Impotance of Total_Trans_Amt for all models If we compare the importance of Total_Revolving_Bal in 1.7, we see a huge difference between tree based models and regression models. The drop-out loss for the first ones is around 20 times lower than for the latter. Figure 1.7: Permutational Feature Impotance of Total_Revolving_Bal for all models Figure 1.8: Permutational Feature Impotance of Gender for all models Figure 1.9: Permutational Feature Impotance of Avg_Utilization_Ratio for all models We can also examine some of the less important features such as Gender (see 1.8 ) and Avg_Utilization_Ratio (see 1.9). In comparison to regression models importance of these variables in XGBoost and Random forest is neglectable. Therefore, we conclude that although the effects of L1 regularization in logistic regression are observable, tree-based models such as XGBoost and Random Forest select the most important features more restrictively. 1.1.1.2 PDP profiles We created Partial Dependence Plots of all variables in the dataset for XGBoost, Random Forest and Logistic Regression with L1 models. Many of the plots turned out to be a horizontal line located on the level of the mean prediction of the models. An example of such a variable is shown in 1.10, predictions of models does not change with the change of Gender. However, features that have high importance do have more complex plots. One can observe prediction varying with the change of Total_Trans_Amt, Total_Revolving_Bal or Total_Ct_Chng_Q4_Q1. Figure 1.10: Partial Dependence Plots of chosen features What we find interesting in 1.10 is an unobserved earlier effect of the Contacts_Count_12_mon variable. The plot is steady for values 1-5 and raises rapidly when the feature takes the value of 6. We examined this case and figured out, that only approx. 0.58% of all observations have value 6 in Contacts_Count_12_mon column. What is more, all of them describe attrited customers. We concluded there are two possible solutions: The dataset is not balanced for this feature. Indeed, the 6th contact with the bank representative is a breakthrough in the relationship with the customer. 1.1.1.3 ALE profiles Figure 1.11: Accumulated-local Profiles Plots of chosen features Accumulated-local Profiles for XGBoost, Random Forest and Logistic Regression with L1 were calculated. The results for chosen variables are shown in 1.11. ALE plots seem to be very similar to PDP profiles. It may suggest there are no interactions between variables in the models. To examine that we plotted both PDP and ALE profiles in 1.12, 1.13. We skip these plots for Logistic Regression because, by definition, there are no variables interactions in this class of models. ALE and PDP plots are parallel thus models detected no interactions between features and they are additive. Figure 1.12: Accumulated-local Profiles and Partial Dependence Profiles Plots of chosen features for XGBoost Figure 1.13: Accumulated-local Profiles and Partial Dependence Profiles Plots of chosen features for Random Forest "],["ml-in-predition-of-real-estate-prices.html", "1.2 ML in predition of real estate prices", " 1.2 ML in predition of real estate prices Authors: Sebastian Deręgowski, Maciej Gryszkiewicz, Paweł Morgen (Warsaw University of Technology) 1.2.1 Abstract Lorem ipsum 1.2.2 Introduction Lorem ipsum 1.2.3 Related Work Lorem ipsum 1.2.4 Methodology Lorem ipsum 1.2.5 Results Lorem ipsum 1.2.6 Summary and conclusions Lorem ipsum "],["xai-heart-disease.html", "1.3 How not to have broken heart &lt;3", " 1.3 How not to have broken heart &lt;3 Authors: Przybyłek Paulina, Rólkiewicz Renata, Słowakiewicz Patryk 1.3.1 Introduction "],["xai1-explainable-wine.html", "1.4 Wines", " 1.4 Wines Authors: Jakub Kosterna, Bartosz Siński, Jan Smoleń "],["xai1-explainable-hotels.html", "1.5 eXplaining predictions of booking cancelations", " 1.5 eXplaining predictions of booking cancelations Authors: Mateusz Krzyziński, Anna Urbala, Artur Żółkowski (Warsaw University of Technology) 1.5.1 Introduction One of the biggest problems and challenges facing the hospitality industry is the significant number of canceled reservations. Common reasons for cancellations include sudden deterioration in health, accidents, bad weather conditions, schedule conflicts or unexpected responsibilities (Falk and Vieru 2018). Interestingly, a noticeable group consists of customers who, after making a reservation, are still looking for new, better offers, and even make many reservations at the same time to be able to choose the most advantageous one (Antonio et al. 2017). The hospitality industry’s response to the above problem are hotel cancellation policies. They play a crucial role in determining various aspects of the hotel business, including the ultimate goal of revenues and profits optimisation. In recent years (before the pandemic), there has been a clear tightening of these policies. Hotels do this, for example by shortening the free cancellation windows or increasing cancellation penalties (Riasi et al. 2019; Smith et al. 2015). The use of machine learning to forecast and identify potential cancellations is also playing an increasing role. There are many systems to support hotel management that use booking data. Various machine learning algorithms are used for this purpose, ranging from support vector machines, through artificial neural networks, to the most common tree-based models (Andriawan et al. 2020; Sánchez-Medina and C-Sánchez 2020). Most of the solutions and projects are only theoretical, while some have been tested in practice, enabling cancellations to be reduced by up to 37 percentage points (Antonio et al. 2019). Unfortunately, most papers do not tackle the issue of the importance of the used explanatory variables and do not try to explain the model’s predictions. However, it is the exploration of trained models that should be treated as one of the key factors in the design of hotel management support systems. Business validation and ethical verification of solutions is necessary. Bearing in mind that a strict cancellation policy or overbooking strategy can have negative effects on both reputation and revenue, systems designers should be wary of unfair biased behaviour. At the same time, the use of explanatory artificial intelligence methods is helpful in creating models with better performance scores. In the following chapter, we present an analysis of predictive models for hotel bookings cancellations. We answer questions about the reasons for the model prediction both in general view and in relation to individual reservations. 1.5.2 Dataset and models 1.5.3 Local explanations In order to explain model output for a particular guest and their booking, we used instance-level exploration methods, such as Break-down, SHAP (SHapley Additive exPlanations), LIME (Local Interpretable Model-agnostic Explanations), CP (Ceteris Paribus). We decided to investigate noteworthy predictions, i.e. false positive and false negative (respectively canceled bookings predicted as not canceled and vice versa), the most valid (the predictions the model was most sure of), and the closest to decision boundary. 1.5.3.1 False positive and false negative predictions We might discover that the model is providing incorrect predictions. The key is to find the reasons for this, that is, to answer the question what has driven the wrong prediction. We used local explanations methods for the observations in both groups with the worst predictions, i.e. the lowest probability of proper classification. Figure 1.14: A plot of Shapley values for random forest model and misclassified observation (false positive) with the highest probability of cancellation. The green and red bars correspond to the contribution of the variable to the prediction. The green ones increase the probability of cancellation, while the red ones decrease it (increase the probability of no cancellation). On the x-axis, there is the model prediction value, while on the y-axis there are variables and their values for the observation. Informations with the biggest contribution to the final prediction are guest’s country of origin (Portugal), a total number of special requests equals zero, and the fact that customer type related to the given booking is Transient-Party. This is an indication that the model may be slightly biased due to the country of origin. It is the property of the customer and not of the booking itself. Thus, depending on the application, it is worth considering whether this response is satisfactory and meets ethical standards. With every value contributes to the misprediction, the only feature with the correct contribution is customer type (Transient-Party guests are the most popular type of customers, accounting for as much as 75% of bookers). Figure 1.15: A plot of Shapley values for random forest model and misclassified observation (false negative) with the lowest probability of cancellation. The elements of the plot have the same meaning as in the previous case. It can be seen that the largest contributions are related to the country of origin of the booker and the type of guest assigned to them. It is worth noting that the values of these variables are the same as in the case of the observation analyzed above. Again, they contribute to the same side of the prediction, but the contribution values are different. In this case, the type of client turns out to be the most important. Most of the values also affect the prediction of no cancellation. It is interesting that a slightly later booking in relation to the date of stay (lead time) has the opposite effect than in the previous example. The reason is the dependencies between the variables. Figure 1.16: A plot of LIME model values for the random forest model and the most misslassified observations. The similarity between the observations is also noticeable in the lime method. The first five variables are identical and have almost the same coefficients. Therefore, it is these less significant variables that influence the final prediction. In the case of these two observations with similar characteristics, but completely different predictions, the use of the SHAP method (generalizing the breakdown method) gives a better picture. The Glass-box model selected in LIME method to approximate the black-box model, and not the data themselves, is not able to capture dependencies between variables. 1.5.3.2 The most valid predictions The considered model returns an appropriate prediction in over 89% of cases. However, the level of certainty of the model with respect to the prediction (i.e. the probability that an observation is assigned to a class) may be different. Thus, it is worth considering why the model is almost sure of some outputs and how would the model’s predictions change if the values of some of the explanatory variables changed. We used local explanations methods for the observations in both groups with the best predictions, i.e. the highest probability of proper classification (equal to 1.0). Figure 1.17: A plot of Shapley values for random forest model and observation with sure negative prediction. The elements of the plot were described above. Like in the previous examples - the largest contribution has the country, in that case: France. Again, this is a Transient-Party customer and that also affected the prediction. Also, no special requests affect negatively to prediction (more than eg. no previous cancellations). Only one of the top variables affected positively: it was no required car parking spaces, but this impact was unnoticeable in the final prediction. Figure 1.18: A plot of Shapley values for random forest model and observation with sure positive prediction. The elements of the plot were described above. Again, Portugal as a country of origin affected positively the probability of cancelation (keep in mind that the hotel is in Portugal, so we can assume that compatriots cancel their reservations more often). Also, no special requests affected positively on prediction (although in the previous case it had a negative effect). We can notice that for positive prediction other factors have the biggest impact than for negative. Eg. a longer lead time moved up to third place and now has a positive impact. Figure 1.19: Ceteris-paribus profiles for the selected continuous explanatory variables and label encoded country variable for the random forest model and observations with the sure prediction. Dots indicate the values of the variables and the values of the predictions for observations. Green profiles are for sure positive prediction (a cancellation), while blue profiles are for sure negative prediction. Looking at the ceteris paribus profiles, it is intuitive to see that the prediction for the observation classified as the not canceled stay is more stable, i.e. less sensitive to changes in the values of explanatory variables. In the case of observation of canceled reservations, a change of the arrival date by a few weeks would cause a significant decrease in the certainty of the prediction. It is related to the seasonality of bookings (the decrease occurs at the beginning of July - the holiday period). However, the biggest changes in the prediction for this observation could be due to noting the fact of additional booking requirements (required car parking spaces and a total number of special requests). Changing these values to non-zero would change the prediction completely. Moreover, the huge changes depend on the country of origin of the booker, which in this case is Portugal. When considering the prediction for an observation classified as not canceled, we see that the only explanatory variable whose change would have a significant impact on the certainty of the prediction is the number of previously canceled reservations. A change to any non-zero value would change the prediction, but its certainty would be close to the decision boundary. 1.5.3.3 The closest to decision boundary predictions We analyzed the situations when the model is sure of the returned output. However, the observations for which the prediction was uncertain, close to the decision limit, are also worth considering. We might want to know the answer to the question of whether it is a matter of similar numbers of explanatory variables shifting the prediction in different directions, or maybe there are variables that do not fit the whole picture, and therefore the model is not certain. It is also worth checking how much such predictions fluctuate depending on the changes in the explanatory variables. Figure 1.20: A plot of Shapley values for random forest model and observation classified as negative with probability near 50%. The elements of the plot were described above. This is a very interesting example. One variable fixed prediction. Very short lead time (one day) opposed all other factors like Portugal as country of origin or no special requests and made the model predict correctly. It is amazing, that one factor can change everything. Figure 1.21: A plot of Shapley values for random forest model and observation classified as positive with probability near 50%. The elements of the plot were described above. This observation is not so exciting as the previous one, but it is the next evidence that the special requests decrease the probability that the client will cancel the reservation. Nevertheless, this observation was classified as positive. Agent was the most important positive variable although in the previous examples he did not have such a contribution. But… agent has that huge contribution only in Shapley. Figure 1.22: A plot of LIME model values for the random forest model and the same observation. Here agent has much less impact. This is a reminder for us that each method works differently and takes different variables into account. It is worth remembering this. In this method for that (and a lot of other) observation the most important factor is no previous cancelations, but it is not enough for the model to make a negative decision. 1.5.4 Global explanations The second group of methods of explainable artificial intelligence are those concerning not a single observation, but the entire set of them. We used these model level explanations to provide information about the quality of the model performance and infer how the model behaves in general. The methods we used for this purpose are Permutational Variable Importance, PDP (Partial Dependence Profile), ALE (Accumulated Local Effects). We applied them not only to the main random forest model, but also to other trained models to compare the obtained results. 1.5.4.1 Importance of explanatory variables First, we decided to check which variables are important for our main model and compare the obtained results with the intuitions we had after conducting the exploration at the prediction level. Figure 1.23: A plot of variable importance. The length of each bar represents the difference between the loss function (1-AUC) for the original data and the data with the permuted values of a particular variable. As we can see above, the most important variable for the model is the information about the guest’s country of origin. This confirms the intuitions obtained thanks to local explanations, for many observations this variable was the key aspect. In particular, the origin of Portugal (the country where the hotels are located) is the most significant for the prediction. The method indicated that the next principal variables for predictions are information on lead time and number of special booking requests. Lead time is the number of days that elapsed between the entering date of the booking into the system and the arrival date. It is a factor that may inform about whether the stay was planned long before or it is spontaneous. Meanwhile, the number of special requests is related to additional interest in the booking, which may indicate that it is an important stay for the client. The next variables with a significant average impact on the model are the group of those concerning the booking method (agent and market segment) and the type of booking (customer type and adr associated with the booking cost). It should also be noted that some of the variables are of marginal importance. These are (from the bottom of the plot) the type of hotel (recall that the data relate to two hotels of different specificity), the number of adults, the number of previously not canceled bookings (this is probably also related to a small number of observations with a non-zero value of this feature - 3.1% in the entire dataset), the type of room reserved. These are the variables that should be considered to be excluded in order to simplify the model. On the other hand, it is quite surprising that the number of previously canceled reservation, which was often indicated by LIME as one of the most important factors for the predictions under consideration, is so insignificant (0.007 drop-out loss) according to the algorithm of the permutational variable importance. 1.5.4.2 Comparison with other models Plots similar to that above in Figure 1.23 are useful for comparison of a variables’ importance in different models. It may be helpful in generating new knowledge - identification of recurring key variables may lead to the discovery of new factors involved in a particular mechanism (Biecek and Burzykowski 2021) (in our case, cancellation of reservations). Thus, we decided to compare the importance of the variables in the models we had trained, described in the Dataset and models section. The plots below show the results for our main Random Forest model and 4 other models with legend: green - Random Forest with label encoding (main model), red - Random Forest with one hot encoding, purple - Logistic Regression, blue - Decision Tree, orange - XGBoost. Figure 1.24: The comparison of the importance of explanatory variables for selected models. The elements of a single plot were described above. Note the different starting locations for the bars, due to differences in the AUC score value obtained for the training dataset for different models. It can be seen that the importance of the variables is related to the type of algorithm used in a given model. For example, in a decision tree, each variable has a clearly noticeable effect on the predictions. In all the tree-based models explained, the most important variable overlaps - it is the aforementioned country of origin. In logistic regression model, this feature is the third most important variable, but it is the model with the worst score overall. However, in general, the groups of the most important variables in each model are also similar. So our earlier conclusions regarding the key booking cancellation factors are confirmed. Likewise, in each of the tree-based models (even in a single decision tree model) the same variables were indicated as least important. An interesting fact is that in random forest with one hot encoding the agent variable is even more important than in random forest with label encoding. It is a categorical variable, so we can see the influence of encoding here - trees could extract more information from this variable thanks to not creating unnatural numerical relationships as with label encoding. The second noticeable difference between these models is the importance of the market segment. 1.5.4.3 The global impact of variables Then we decided to check the influence of the most influential variables on the predictions in the context of the whole set. For this, we used the ALE method. The plots below show the results for our Random Forest and 3 other tree-based models with legend: green - Random Forest with label encoding (main model), red - Random Forest with one hot encoding, blue - Decision Tree, orange - XGBoost. Note that we chose not to generate plots for the logistic regression model because it performed too poorly. Figure 1.25: ALE plot for the country. The ALE plots work like Ceteris Paribus - they show how the variable affects a prediction but not only for one observation - for the entire training dataset. We have a greater probability of resignation for one country - this is Portugal. This confirms our hypothesis built on the basis of local explanatory methods. Generally, compatriots more often resign from booking. Figure 1.26: ALE plot for lead time. The elements of a single plot were described above. And again we have confirmation that the longer the lead time, the greater the chance that the customer will resign. The ALE plots are very similar to Ceteris Paribus but that’s a great example that they’re not the same. Look at the Ceteris Paribus profile for lead time in the Local explanations section. The probability of resignation increased to about 170 days, then decreased and remained at a constant level. Here we can see that it was the “local behavior.” Globally, the probability only grows, then it stabilizes (after about 350 days - a year). Therefore, you may suspect that it is not worth allowing reservations so far in advance. Figure 1.27: ALE plot for total of special requests and required car parking spaces. The elements of a single plot were described above. Additional actions taken by the client regarding booking reduce the likelihood of cancellation. The very first special request significantly reduces the probability of resignation. The influence of the next ones is not that clear. This also confirms the thesis we made earlier that the lack of special requests increases the probability of resignation. Reserving a parking space has an even greater impact on the predictions. We may think that in these hotels it is payable in advance, or that car travelers are less dependent on public transport, so their arrival is more certain. Figure 1.28: ALE plot for previous cancellations and previous bookings not canceled. The elements of a single plot were described above. Information about a given customer’s prior bookings is also very valuable for prediction. The fact of earlier cancellation of a reservation strongly influences the prediction of the next one, which seems natural. A non-zero number of prior non-canceled bookings works the opposite way, but the prediction values don’t fluctuate that much. After analyzing these examples, an important conclusion can be drawn about the XAI methods. ALE plots can be a great tool for analyzing the influence of variables on the prediction - they can be used to verify the hypotheses put forward at the stage of local explanations and introduce new ones. When comparing the results for different models, note that the profiles for random forests with both versions of the categorical variable encoding are almost identical (green and red lines in the graphs). Looking more broadly, the profiles are comparable for all models. The most significant differences can be seen in the variables relating to the previous reservations of a given customer - the number of canceled and non-canceled reservations. The XGBoost model favors non-zero values more - the prediction changes are bigger. In general, this model is the most sensitive to all variables, as can be seen from the shape of the profile curves. Moreover, we used the comparison of the PDP and ALE plots for our main model (see Figure 1.29). In the case of some variables, the profiles generated using both methods almost coincide. However, there are also variables where you can see differences in prediction values, but profiles are parallel to each other. This parallelism suggests and allows us to conclude that the used model is additive due for these explanatory variables (Biecek and Burzykowski 2021). Figure 1.29: Partial-dependence and accumulated-local profiles for the main random forest model and selected variables 1.5.5 Summary and conclusions References Andriawan, Z. A., Purnama, S. R., Darmawan, A. S., Ricko, Wibowo, A., Sugiharto, A., &amp; Wijayanto, F. (2020). Prediction of hotel booking cancellation using CRISP-DM. In 2020 4th international conference on informatics and computational sciences (ICICoS) (pp. 1–6). https://doi.org/10.1109/ICICoS51170.2020.9299011 Antonio, N., Almeida, A. de, &amp; Nunes, L. (2017). Predicting hotel booking cancellations to decrease uncertainty and increase revenue. Tourism &amp; Management Studies, 13(2), 25–39. https://doi.org/10.18089/tms.2017.13203 Antonio, N., Almeida, A. de, &amp; Nunes, L. (2019). An automated machine learning based decision support system to predict hotel booking cancellations. Data Science Journal, 18(1), 1–20. https://doi.org/10.5334/dsj-2019-032 Biecek, P., &amp; Burzykowski, T. (2021). Explanatory Model Analysis. Chapman; Hall/CRC, New York. https://pbiecek.github.io/ema/ Falk, M., &amp; Vieru, M. (2018). Modelling the cancellation behaviour of hotel guests. International Journal of Contemporary Hospitality Management, 30(10), 3100–3116. https://doi.org/10.1108/ijchm-08-2017-0509 Riasi, A., Schwartz, Z., &amp; Chen, C.-C. (2019). A paradigm shift in revenue management? The new landscape of hotel cancellation policies. Journal of Revenue and Pricing Management, 18(6), 434–440. https://doi.org/10.1057/s41272-019-00189-3 Sánchez-Medina, A. J., &amp; C-Sánchez, E. (2020). Using machine learning and big data for efficient forecasting of hotel booking cancellations. International Journal of Hospitality Management, 89, 102546. https://doi.org/10.1016/j.ijhm.2020.102546 Smith, S. J., Parsa, H. G., Bujisic, M., &amp; Rest, J.-P. van der. (2015). Hotel cancelation policies, distributive and procedural fairness, and consumer patronage: A study of the lodging industry. Journal of Travel &amp; Tourism Marketing, 32, 886–906. https://doi.org/10.1080/10548408.2015.1063864 "],["explainable-artificial-intelligence-2.html", "Chapter 2 Explainable Artificial Intelligence 2", " Chapter 2 Explainable Artificial Intelligence 2 "],["deep-learning-1.html", "Chapter 3 Deep Learning 1", " Chapter 3 Deep Learning 1 Author: Weronika Hryniewska Deep learning is one of the most rapidly developing field in artificial intelligence. Problems that previously required a lot of features engineering became easily solvable. New possibilities opened, and deep learning has started to adopt in various domains. One of the most demanding disciplines is medicine. As a result of the outbreak of the COVID-19 pandemic, many scientists became interested in the possibilities of deep learning application in radiology. Many solutions have been created for classification, segmentation and detection based on computed tomography and radiographs of the lungs. During classes, we explored deep learning methods for computer vision. If you would like to read more about them, please take a look at books: “Deep Learning with Python” (Chollet 2017) and “Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems” (Géron 2017). We focused on results reproduction and further development of the available code of the following papers: 1. LungNet (Anthimopoulos et al. 2019) 2. BCDU-Net (Azad et al. 2019) (Asadi-Aghbolaghi et al. 2020) 3. DeepCOVIDExplainer (Karim et al. 2020) 4. ERSCovid (S. Wang et al. 2020) 5. COVID-Net (L. Wang et al. 2020) References Anthimopoulos, M., Christodoulidis, S., Ebner, L., Geiser, T., Christe, A., &amp; Mougiakakou, S. (2019). Semantic segmentation of pathological lung tissue with dilated fully convolutional networks. IEEE Journal of Biomedical and Health Informatics, 23(2), 714–722. https://doi.org/10.1109/JBHI.2018.2818620 Asadi-Aghbolaghi, M., Azad, R., Fathy, M., &amp; Escalera, S. (2020). Multi-level context gating of embedded collective knowledge for medical image segmentation. https://arxiv.org/abs/2003.05056 Azad, R., Asadi-Aghbolaghi, M., Fathy, M., &amp; Escalera, S. (2019). Bi-directional ConvLSTM u-net with densley connected convolutions. In Proceedings of the IEEE/CVF international conference on computer vision (ICCV) workshops. Chollet, F. (2017). Deep learning with python. Manning. Géron, A. (2017). Hands-on machine learning with scikit-learn and TensorFlow : Concepts, tools, and techniques to build intelligent systems. O’Reilly Media. Karim, M. R., Döhmen, T., Rebholz-Schuhmann, D., Decker, S., Cochez, M., &amp; Beyan, O. (2020). DeepCOVIDExplainer: Explainable COVID-19 diagnosis from chest x-ray images. IEEE. Wang, L., Lin, Z. Q., &amp; Wong, A. (2020). COVID-net: A tailored deep convolutional neural network design for detection of COVID-19 cases from chest x-ray images. Scientific Reports, 10(1), 19549. https://doi.org/10.1038/s41598-020-76550-z Wang, S., Zha, Y., Li, W., Wu, Q., Li, X., Niu, M., et al. (2020). A fully automatic deep learning system for COVID-19 diagnostic and prognostic analysis. European Respiratory Journal, 56(2). https://doi.org/10.1183/13993003.00775-2020 "],["bcdunet.html", "3.1 BCDUNet", " 3.1 BCDUNet Authors: Maria Kałuska, Paweł Koźmiński, Mikołaj Spytek 3.1.1 Abstract Some text will be here in the future "],["an-exploration-of-deepcovidexplainer-explainable-covid-19-diagnosis-from-chest-x-rays.html", "3.2 An Exploration of DeepCovidExplainer: Explainable COVID-19 Diagnosis from Chest X-rays", " 3.2 An Exploration of DeepCovidExplainer: Explainable COVID-19 Diagnosis from Chest X-rays Authors: Kurowski Kacper, Mróz Zuzanna, Podsiad Aleksander 3.2.1 Introduction "],["covid-net.html", "3.3 COVID-Net", " 3.3 COVID-Net Authors: Jakub Kozieł, Tomek Nocoń, Kacper Staroń 3.3.1 Introduction "],["deep-learning-2.html", "Chapter 4 Deep Learning 2", " Chapter 4 Deep Learning 2 Author: Paulina Tomaszewska Artificial Intelligence (AI) especially Deep Learning (DL) is a rapidly emerging field. It is proved by the number of publications – every day some new paper is released. In the spirit of “open science” (Mendez et al. 2020) not only papers are published in journals but also are available previously as preprints. This helps in the fast exchange of knowledge between researchers. In order to fasten progress in the field even more, it is recommended to open source also the code as well as data used for analysis in the paper. In such a scenario, researchers inspired by someone’s papers will not have to implement the described solution independently but rather focus on adding improvements. Such a pipeline, however, requires the reproducibility of the results shown in the paper. It means that by running the code given by the authors, the same results as described in the paper should be obtained. People started to verify the reproducibility of the papers also to check whether the results in the paper are trustworthy. It happens that the authors do “cherry-picking” of the results. The reproducibility of the papers is getting more and more attention. There is a web page (Yildiz et al. 2021) where the outcomes of the paper reproducibility studies are stored. In this chapters, students focused on the reproducibility of Deep Learning papers (Liu et al. 2020). It was motivated by two facts featuring models in Deep Learning: they are complex (often have an immense number of parameters) they have an inherent component of randomness (e.g. weight initialization, data augmentation) These two points show that the task of reproducibility in Deep Learning can be sometimes a challenge. References Liu, C., Gao, C., Xia, X., Lo, D., Grundy, J., &amp; Yang, X. (2020). On the replicability and reproducibility of deep learning in software engineering. https://arxiv.org/abs/2006.14244 Mendez, D., Graziotin, D., Wagner, S., &amp; Seibold, H. (2020). Open science in software engineering. Contemporary Empirical Methods in Software Engineering, 477–501. https://doi.org/10.1007/978-3-030-32489-6_17 Yildiz, B., Hung, H., Krijthe, J. H., Liem, C. C. S., Loog, M., Migut, G., et al. (2021). ReproducedPapers.org: Openly teaching and structuring machine learning reproducibility. In B. Kerautret, M. Colom, A. Krähenbühl, D. Lopresti, P. Monasse, &amp; H. Talbot (Eds.), Reproducible research in pattern recognition (pp. 3–11). Cham: Springer International Publishing. "],["axondeepseg-automatic-axon-and-myelin-segmentation-from-microscopy-data-using-convolutional-neural-networks.html", "4.1 AxonDeepSeg: automatic axon and myelin segmentation from microscopy data using convolutional neural networks", " 4.1 AxonDeepSeg: automatic axon and myelin segmentation from microscopy data using convolutional neural networks Authors: Jakubowski Mikołaj, Tomaszewski Patryk, Ziemła Mateusz 4.1.1 Introduction "],["ara-cnn-a-bayesian-deep-learning-model-intended-for-histopathological-image-classification-.html", "4.2 ARA-CNN - a Bayesian deep learning model intended for histopathological image classification.", " 4.2 ARA-CNN - a Bayesian deep learning model intended for histopathological image classification. *Authors: Wojciech Szczypek, Jakub Lis, Jan Gąska (Warsaw University of Techcnology) "],["rethinking-the-u-net-architecture-for-multimodal-biomedical-image-segmentation.html", "4.3 Rethinking the U-Net architecture for multimodal biomedical image segmentation", " 4.3 Rethinking the U-Net architecture for multimodal biomedical image segmentation Grudzień Adrianna, Łukaszyk Marcin, Piasecki Michał 4.3.1 Introduction Tutaj będzie tekst kiedyś "],["dl2-rmdl-unet.html", "4.4 Analyzing Reproducibility churns", " 4.4 Analyzing Reproducibility churns Authors: Marceli Korbin, Szymon Szmajdziński, Paweł Wojciechowski (Warsaw University of Techcnology) "],["title.html", "4.5 Title", " 4.5 Title Authors: Filip Chrzuszcz, Szymon Rećko, Mateusz Sperkowski (Warsaw University of Technology) 4.5.1 Title, Authors, Abstract, Keywords 4.5.2 Introduction 4.5.3 Related Literature 4.5.4 Methods 4.5.5 Result Skrot do pierwszego projektu: Despite lower results than in the first paper, in most datasets we still achieved better results than the baselines paper attempted to beat. The ones that we weren’t able to reproduce where either limits of processing power, or could be assigned to effect off randomness which is basis od this paper. The authors unfortunately didn’t include their randomness results, therefore their exact calculations aren’t reproducible. 4.5.6 Discussion 4.5.7 Conclusion 4.5.8 References 4.5.9 Random Multimodel Deep Learning for Classification Results .tg {border-collapse:collapse;border-spacing:0;} .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Dataset WOS-5736 WOS-11967 WOS-46985 Reuters-21578 Score Source Paper Repr. Paper Repr. Paper Repr. Paper Repr. RMDL 3 RDLs 90.86 89.37 87.39 84.25 78.39 — 89.10 87.64 9 RDLs 92.60 89.28 90.65 — 81.92 — 90.36 89.83 15 RDLs 92.66 — 91.01 — 81.86 — 89.91 — 30 RDLs 93.57 — 91.59 — 82.42 — 90.69 — Table 1 4.5.9.1 Reuters-21578 Paper’s Plots Our Reproduction Figure 1 4.5.9.2 WOS-5736 Paper’s Plots Our Reproduction Figure 2 .tg {border-collapse:collapse;border-spacing:0;} .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-9wq8{border-color:inherit;text-align:center;vertical-align:middle} .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Dataset IMDB 20NewsGroup Score Source Paper Repr. Paper Repr. RMDL 3 RDLs 89.91 88.49 86.73 — 9 RDLs 90.13 — 87.62 — 15 RDLs 90.79 — 87.91 — Table 2 ERROR RATE 1-Accuracy .tg {border-collapse:collapse;border-spacing:0;} .tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; overflow:hidden;padding:10px 5px;word-break:normal;} .tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px; font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;} .tg .tg-9wq8{border-color:inherit;text-align:center;vertical-align:middle} .tg .tg-c3ow{border-color:inherit;text-align:center;vertical-align:top} .tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top} Dataset MNIST CIFAR-10 Score Source Paper Repr. Paper Repr. RMDL 3 RDLs 0.51 0.55 9.89 38.23 9 RDLs 0.41 0.65 9.1 36.91 15 RDLs 0.21 — 8.74 — 30 RDLs 0.18 — 8.79 — Table 3 4.5.9.3 CIFAR 10 Paper’s Plots Our Reproduction Figure 3 4.5.9.4 MNIST Paper’s Plots Our Reproduction Figure 4 4.5.10 Adversarial Attacks Against Medical Deep Learning Systems "],["machine-learning.html", "Chapter 5 Machine Learning", " Chapter 5 Machine Learning Author: Hubert Baniecki An ever-growing domain of machine learning decision systems in medicine has crossed ways with the COVID-19 pandemic. Precariously, a vast majority of the proposed predictive models focus on achieving high performance; while overlooking comprehensive validation. Nowadays, providing representative data, model explainability, even bias detection become mandatory for responsible prediction making in high-stakes medical applications. The following short papers introduce new views into the already published work on the topic of patients’ COVID-19 mortality prognosis using supervised machine learning: Validation and comparison of COVID-19 mortatility prediction models on multi-source data. Michał Komorowski, Przemysław Olender, Piotr Sieńko, Konrad Welkier One model to fit them all: COVID-19 survival prediction using multinational data. Marcelina Kurek, Mateusz Stączek, Jakub Wiśniewski, Hanna Zdulska Transparent machine learning to support predicting COVID-19 infection risk based on chronic diseases. Dawid Przybyliński, Hubert Ruczyński, Kinga Ulasik Comparison of neural networks and tree-based models in the clinical prediction of the course of COVID-19 illness. Jakub Fołtyn, Kacper Grzymkowski, Konrad Komisarczyk "],["validation-and-comparison-of-covid-19-mortatility-prediction-models-on-multi-source-data.html", "5.1 Validation and comparison of COVID-19 mortatility prediction models on multi-source data", " 5.1 Validation and comparison of COVID-19 mortatility prediction models on multi-source data Authors: Michał Komorowski, Przemysław Olender, Piotr Sieńko, Konrad Welkier 5.1.1 Abstract The work of (Yan et al. 2020a) from the first months of the COVID-19 pandemic laid the foundations for further research in the area of machine learning models for patients classification by introducing a simple decision tree that in the opinion of the inventors resolved the whole issue. Since that time a few papers have emerged that touch upon the same case in which other reseachers tested this decision tree on their datasets. Their findings that the original model is not suitable for patients from other countries than China appeared interesting to us and hence in the following paper we present results of our work which aim was to build models on each of the considered datasets as well as on all of them combined in order to find an universal approach for classification of patients from various countries. After testing various models such as XGBoost, Logistic Regression, SVM and Tabnet we came up with the conclusion that there is no one model for all of the datasets that includes only at most 5 crucial variables. 5.1.2 Introduction 5.1.3 Data description 5.1.4 Comparison of the models 5.1.5 Results References Yan, L., Zhang, H.-T., Goncalves, J., Xiao, Y., Wang, M., Guo, Y., et al. (2020a). An interpretable mortality prediction model for COVID-19 patients. Nature Machine Intelligence, 2(5), 283–288. https://doi.org/10.1038/s42256-020-0180-7 "],["one-model-to-fit-them-all-covid-19-survival-prediction-using-multinational-data.html", "5.2 One model to fit them all: COVID-19 survival prediction using multinational data", " 5.2 One model to fit them all: COVID-19 survival prediction using multinational data Authors: Marcelina Kurek, Mateusz Stączek, Jakub Wiśniewski, Hanna Zdulska 5.2.1 Abstract During the outbreak of SARS-CoV-2 many scientists tried to build a model that was able to predict the survival or death of patients based on available medical data. Yan et al. (2020b) were among the first researchers to introduce their model based on blood data (lactic dehydrogenase (LDH), lymphocyte percentage, and high-sensitivity C-reactive protein (hs-CRP)) with 0.90 accuracy, however, recreations of this model trained on other countries’ data - the US, Netherlands, and France were not so successful. In this article, we explore the possibility of building an international model for predicting COVID-19 survival. We focused on exploring the models, their variable importance, analyzed the bias they introduced, and concluded with guidelines for future researchers working on this topic. 5.2.2 Introduction Machine learning models are becoming popular in medicine because of the various opportunities they create. Such algorithms may be useful in performing early diagnosis, assessing disease severity, or personalizing treatment. During the COVID-19 pandemic, there were numerous possibilities associated with machine learning models. For example, an algorithm could predict which patients should be qualified for the Intensive Care Unit or who should be treated under a respirator. Additionally, due to the worldwide character of COVID-19 pandemic, it was easier to gather data about symptoms and various blood measures from thousands of patients. During this project, we have been working with the article “An interpretable mortality prediction model for COVID-19 patients” by Yan et al. (2020b). The article presents a decision tree, which predicts whether a patient will die or survive the disease based on the level of lactic dehydrogenase, C-reactive protein, and lymphocytes in blood samples. The presented model obtained high accuracy and ROC AUC scores on data from Yan et al., but had poor scores on datasets from the Netherlands and the US. According to the article ‘A study in transfer learning: leveraging data from multiple hospitals to enhance hospital-specific predictions’ Wiens et al. (2014) increasing the number of dataset sources for machine learning models may lead to poor performance. With a large number of datasets, controlling the extent to which data from each hospital contribute to the final model is complicated. As a result, procedures for identifying an optimal setting for hyperparameters can quickly become inefficient. In our research, entitled ‘One model to predict them all,’ we focused on answering the question if creation of the model with satisfactory behavior and performance independent of the data origin is possible. This approach could have led us to create an international model, which could help in recognizing the severity of COVID-19 cases. Contrastingly, creating such an algorithm could be impossible for many reasons, for example conducting different examinations in hospitals or different medical standards. 5.2.3 Data sources To create an international model, we used data from three different sources: China, New York, and the Netherlands. Dataset from China is added to the article ‘An interpretable mortality prediction model for COVID-19 patients’ by Yan et al. (2020b). It contains 375 observations of LDH, CRP, and Lymphocytes percentage, along with the outcome of the COVID-19 disease. Dataset from the Netherlands is attached to the article ‘Replication of a mortality prediction model in Dutch patients with COVID-19’ by Quanjel et al. (2021). It contains 306 observations. Apart from information about blood samples, the age and gender of patients are also provided. The New York dataset is not attached to the article ‘External validation demonstrates the limited clinical utility of the interpretable mortality prediction model for patients with COVID-19’ by Barish et al. (2021), as it contains confidential data. However, due to the civility of the authors, we were provided with the dataset. The New York dataset contains 1000 observations of blood samples, which is more than in both other datasets combined. 5.2.4 Model building First, a model performing very well on training data can perform poorly on new observations coming from a different location. In our case, we found that the model presented by Yan et al. (2020b) is not portable and does perform poorly on data from New York and the Netherlands. However, models trained on such a dataset tend to include the source of data as an important feature (as in Figure 5.1). That means the model is biased for some or all sources of data to give better predictions. Simultaneously it fails to achieve scores as good as models dedicated for each country. Unfortunately, such models have low scores in different performance metrics and are worse than a model created specifically for a given source of data. As an example, we used LazyPredict twice to compare scores of various models trained on data from all 3 sources: the first run was on data without the column containing the source of data and a second run was on data containing all columns. The top 3 models from both runs are presented in the tables below (results were sorted by “ROC AUC”). Table 1: Scores from the LazyPredict for the top 3 classifiers sorted by “ROC AUC” trained on data from 3 sources excluding the column containing information about the source of data Model Accuracy Balanced Accuracy ROC AUC F1 Score Time Taken AdaBoostClassifier 0.753 0.714 0.714 0.751 0.191 NearestCentroid 0.700 0.702 0.702 0.708 0.023 KNeighborsClassifier 0.726 0.689 0.689 0.725 0.038 Table 2: Scores from the LazyPredict for the top 3 classifiers sorted by “ROC AUC” trained on data from 3 sources including the column containing information about the source of data. Model Accuracy Balanced Accuracy ROC AUC F1 Score Time Taken RandomForestClassifier 0.784 0.740 0.740 0.779 0.326 XGBClassifier 0.770 0.735 0.735 0.768 0.223 LabelPropagation 0.767 0.729 0.729 0.765 0.114 When the source of data is excluded from the training dataset, the results look less promising. This is expected as the origin of data proved to be a useful feature. Next, we tuned the parameters of the top models from each table with grid search and checked their scores using the dalex (Baniecki et al. 2020) explainer. RandomForestClassifier and AdaBoostClassifier scored 0.83 and 0.76 ROC AUC respectively which proves the hypothesis about the influence of the origin. Our next step was to explore the ways to measure the effect of training on the data from certain countries. As we have proved, the origin is important in the modeling. Whether it is the effect of the healthcare system, biological differences between people, or the hospitals they were in, it may influence the model in ways that may not be clearly predicted. To determine how important the place of origin is, we trained RandomForestClassifier on data with the said place. We can see in the figure below that the two most important features are blood-related. However, the third most important feature is the information whether the patient was from China or not. Figure 5.1: Variable importance for RandomForestClassifier The variable importance here is measured with perturbations (Fisher et al. 2019a). The idea behind it is that we first measure the performance on the entire model. Secondly, we reorder elements of a column (variable), train model, and then measure performance again. The difference in performances is called drop-out loss and it depicts how important the variables are. This information was not surprising, however, we also made a similar test that conveniently simplifies the model. We created a surrogate decision tree from the earlier classifier. The surrogate model is trained to approximate the predictions of a black box predictor (Molnar 2019). Figure 5.2: Surrogate model for RandomForestClassifier As expected the variable indicating the source of the data was among the three most important splits. We also had concerns over the bias introduced by said sources. The bias (or fairness) of the classifier is discrimination in decisions made by the model. The kind of fairness that we will focus on is called group fairness and it concerns the difference in outcomes between groups of people. There are many ways to measure this bias with so-called fairness metrics. They all can be derived from confusion matrices for different subgroups. We will focus on five of them that are used in Fairness check (Wiśniewski and Biecek 2021). With the help of Equal Opportunity (TPR) (Hardt et al. 2016), Predictive Parity (PPV) (Chouldechova 2016), Predictive Equality (FPR) (Corbett-Davies et al. 2017), Statistical Parity (STP) (Dwork et al. 2012), and Accuracy Equality (ACC) (Berk et al. 2017). The Fairness check detects bias in metrics via the four-fifths rule (Code of Federal Regulations 1978). It simply looks at metrics for the privileged subgroup (in this case whether data comes from China) and for unprivileged subgroups and calculates their ratio. If this ratio is within (0.8, 1.25) then we assume that there is no bias. To investigate this claim we trained two machine learning models. The first one was XGBoost with the same parameters as in Yan et al. (2020b). The results were quite surprising as the model introduced bias in 4 metrics. Figure 5.3: Fairness check on XGBoost. The model has bias present in four metrics. To make sure that the model did not overfit the data and gave steady predictions we also checked the fairness of the Histogram-based Gradient Boosting Classification Tree from the scikit-learn package (Pedregosa et al. 2011). The bias was indeed lower but still significant. Therefore we also decided to use bias mitigation strategies. To do this, we firstly merged some subgroups for the algorithms to work better. We tried to make “fair classifiers” with two Python packages fairtorch and fairlearn (Bird et al. 2020). They are related to each other as the fairtorch implements the solutions from fairlearn. Using those in-processing algorithms (these are the kind of mitigation approaches that reduce the bias during model training) we obtained 2 additional models. One of them was a neural net and the other was the Histogram-based Gradient Boosting Classification Tree that was trained using the reductions approach. The amount of bias reduced by the neural net from fairtorch was not satisfying enough and therefore will not be shown here. However, the results from fairlearn were quite good. Figure 5.4: Fairness check on models before and after the reductions. As we can see despite the fact that the reduced model does not fit within the green field we decided that the bias was in fact reduced. The last thing to check was the performance of the said model. Such reductions in the amount of bias may result in a significant drop in performance. In this case, it was the same. The ROC AUC metric dropped from 0.71 to 0.59 which for the medical applications is not enough. Therefore we concluded that in the case of this data the models were biased towards different origins. 5.2.5 Summary and discussion Machine learning can be very useful when applied to medical data. Over the last few years, the increased amount of health-related information created many possibilities to aid medical specialists, for example in Decision Support Systems. Such data can be successfully applied to predicting COVID-19 as proven by Yan et al. (2020b). However, creating a multinational model isn’t trivial for a few reasons. Even in a single country, forms of data collection vary from hospital to hospital. It’s impossible to enforce a unified system and data format across continents. Moreover, some measurements may depend on the time of the day taken - for example, blood pressure will be different in the morning and in the evening. Another problem was observed by Kaushal et al. (2020): Whether by race, gender or geography, medical AI has a data diversity problem: researchers can’t easily obtain large, diverse medical data sets—and that can lead to biased algorithms. The case of having a model that doesn’t take into account the origin of the sample will lead to simplification of such a model since cut-off values were different for data from China, NY, and the Netherlands. This leads to lower metric scores and this excludes it from using in a medical environment, where precision is crucial. We were working on a dataset merged from data coming from different countries and in this setting, machine learning models tend to be biased towards different nationalities. Our attempts at reducing this discrimination were not successful. We believe that with a bigger or more balanced dataset we would have a slightly better chance at meeting our goal. Making models on country levels or geographical regions to achieve maximum fairness is more reasonable. It will also allow scientists to achieve the best results in predicting COVID-19 survival and ultimately we defeat COVID-19 and live happily ever after, till the end of our days, as said by short man(HOBBIT) and acclaimed author Bilbo Baggins. References Baniecki, H., Kretowicz, W., Piatyszek, P., Wisniewski, J., &amp; Biecek, P. (2020). dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python. arXiv:2012.14406. https://arxiv.org/abs/2012.14406 Barish, M., Bolourani, S., Lau, L. F., Shah, S., &amp; Zanos, T. P. (2021). External validation demonstrates limited clinical utility of the interpretable mortality prediction model for patients with COVID-19. Nature Machine Intelligence, 3(1), 25–27. https://doi.org/10.1038/s42256-020-00254-2 Berk, R., Heidari, H., Jabbari, S., Kearns, M., &amp; Roth, A. (2017). Fairness in Criminal Justice Risk Assessments: The State of the Art. Sociological Methods &amp; Research. https://doi.org/10.1177/0049124118782533 Bird, S., Dudík, M., Edgar, R., Horn, B., Lutz, R., Milan, V., et al. (2020). Fairlearn: A toolkit for assessing and improving fairness in AI (No. MSR-TR-2020-32). Microsoft. https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/ Chouldechova, A. (2016). Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments. Big Data, 5. https://doi.org/10.1089/big.2016.0047 Code of Federal Regulations. (1978). SECTION 4D, UNIFORM GUIDELINES ON EMPLOYEE SELECTION PROCEDURES (1978). https://www.govinfo.gov/content/pkg/CFR-2014-title29-vol4/xml/CFR-2014-title29-vol4-part1607.xml Corbett-Davies, S., Pierson, E., Feller, A., Goel, S., &amp; Huq, A. (2017). Algorithmic Decision Making and the Cost of Fairness. https://doi.org/10.1145/3097983.3098095 Dwork, C., Hardt, M., Pitassi, T., Reingold, O., &amp; Zemel, R. (2012). Fairness through awareness. ITCS. https://doi.org/10.1145/2090236.2090255 Fisher, A., Rudin, C., &amp; Dominici, F. (2019a). All Models are Wrong, but Many are Useful: Learning a Variable’s Importance by Studying an Entire Class of Prediction Models Simultaneously. Journal of Machine Learning Research, 20(177), 1–81. http://jmlr.org/papers/v20/18-760.html Hardt, M., Price, E., Price, E., &amp; Srebro, N. (2016). Equality of Opportunity in Supervised Learning. NeurIPS. https://papers.nips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html Kaushal, A., Altman, R., &amp; Langlotz, C. (2020). Health Care AI Systems Are Biased. Scientific American. https://www.scientificamerican.com/article/health-care-ai-systems-are-biased Molnar, C. (2019). Interpretable Machine Learning: A Guide for Making Black Box Models Explainable. https://christophm.github.io/interpretable-ml-book Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., et al. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825–2830. Quanjel, M. J. R., Holten, T. C. van, Gunst-van der Vliet, P. C., Wielaard, J., Karakaya, B., Söhne, M., et al. (2021). Replication of a mortality prediction model in dutch patients with COVID-19. Nature Machine Intelligence, 3(1), 23–24. https://doi.org/10.1038/s42256-020-00253-3 Wiens, J., Guttag, J., &amp; Horvitz, E. (2014). A study in transfer learning: leveraging data from multiple hospitals to enhance hospital-specific predictions. Journal of the American Medical Informatics Association, 21(4), 699–706. https://doi.org/10.1136/amiajnl-2013-002162 Wiśniewski, J., &amp; Biecek, P. (2021). fairmodels: A Flexible Tool For Bias Detection, Visualization, And Mitigation. arXiv:2104.00507. https://arxiv.org/abs/2104.00507 Yan, L., Zhang, H.-T., Goncalves, J., Xiao, Y., Wang, M., Guo, Y., et al. (2020b). An interpretable mortality prediction model for COVID-19 patients. Nature Machine Intelligence, 2(5), 283–288. https://doi.org/10.1038/s42256-020-0180-7 "],["transparent-machine-learning-to-support-predicting-covid-19-infection-risk-based-on-chronic-diseases.html", "5.3 Transparent machine learning to support predicting COVID-19 infection risk based on chronic diseases", " 5.3 Transparent machine learning to support predicting COVID-19 infection risk based on chronic diseases Authors: Dawid Przybyliński, Hubert Ruczyński, Kinga Ulasik 5.3.1 Abstract The moment the COVID-19 outbreak in 2019 occurred people started using machine learning in order to help in managing the situation. Our work is a build up to research described in (Yan et al. 2020c) and in (Bello-Chavolla et al. 2020) articles, we also use the same data in order to create our own models. We improve and create new models predicting COVID-19 mortality based on blood parameters and analyze original models presented in the articles. We train a prediction model with over 70% accuracy calculating risk of getting infected with the virus depending only on person’s sex, age and chronic diseases and created an application which calculates the COVID-19 mortality risk based on input data and explains the model with visualizations. Considering accuracy of the model, calculations based only on chronic diseases may not achieve best results but we propose an approach which doesn’t need any additional medical information, is explainable and easy to understand and requires only data that everyone has access to. 5.3.2 Introduction The COVID-19 pandemic is currently the world’s most biggest problem, affecting not only everyone’s private lives, but also other areas of human activity, including research. In the scientific field, this is a specific issue that scientists are still able to approach this problem in completely different ways, because no distinctive and right path has yet been established. It was in the combination of these three aspects that we saw an opportunity to deepen the current understanding of the pandemic and create a useful tool for predicting mortality from the disease. 5.3.3 Case Study 5.3.4 Flaws As a first step, we started analyzing one of the first articles [1] on the development of predictive models to predict COVID-19 mortality in depth. The main disadvantages of the model presented in it were the very poor prediction of mortality in the case of testing on external data sets [2] [3] and the related allegation of not testing one’s solution on external data [2]. In addition, our team noticed a very large bias present in the original data, thanks to which even the simplest models achieved extremely high efficiency of over 90%. 5.3.5 Improvements Our first approach to the problem was to try to improve the work of the articles authors in question through very different methods. 5.3.5.1 Prediction using original parameters on external data First, we decided to use the broader pandemic knowledge that the authors of the original article did not have to improve the model on its default data. Inspired by a newer article [4], we decided to test how the selection of variables improved according to the latest knowledge will affect the predictions of the predictive models. For the suggested parameters: age, C-reactive protein, chloride, albumin, lymphocyte count and LDH, we created a correlation map to select the most important of them. Correlation heatmap From the above analysis we distinguished that the most important features there are age, albumin, LDH and C protein, which we used in our models. After training and testing the GradientBoostingClassifier and AdaBoostClassifier models on slightly reduced data (the original test set had to be replaced by the test and validation set), we obtained cross-validation precision at the level of 0.979 and 0.958 and the following reports: The above results confirmed the improvement in the quality of the original model, whose precision score for death was only 0.81 and cross-validation stood around 0.97. Thanks to the authors of the paper (5-3-american?), we were given access to additional data with features coincident with those we already had. Dataset contained over 1000 observations and only fourteen features that were selected by owners to fitas best model as possible. In order to test prevoius model (AdaBoost), we used it to predict the outcome for those thousand patients, receiving following results: Results received for external data Model performance has dropped significantly, the outcome is far from desired. 5.3.5.2 PCA In order to analyze data further, we performed Principal Component Analysis. For visualization’s simplification we considered only first two, most substantial components. Obtained two-dimensional plot is shown on Figure 8. Explained variance ratios were: 0.226 (for the first component) and 0.063 for the second component), which results in total explained variance ratio of 0.289 for both components together. Taking absolute values of the scores of each feature from the first component might be also used for feature selection. Those with the highest magnitudes were: ‘Prothrombin activity,’ ‘Lactate dehydrogenase,’ ‘albumin,’ ‘Urea,’ ‘neutrophils(%),’ ‘(%)lymphocyte.’ Some of the features are those, that we have already known are important, such as ‘albumin,’ ‘(%)lymphocyte’ or ‘Lactate dehydrogenase,’ but ‘age’ was around the middle, not among the top ones. Principal Component Analysis The most noticeable fact is that our two classes are almost separable with just a single line. Even without any sort of complex machine learning or other algorithms, it’s possible and not complicated to fit a line that divides cases ended in death from cases followed by patient’s recovery. As an example, same visualization with additional function \\(y=2.5x+2.5\\), created without any sort of optimization techniques, is presented below Principal Component Analysis with line separator Such division achieves (train) accuracy of 0.94, which is almost as good as results received by machine learning algorithms described in the paper, what might encourage to consider given data not authoritative. 5.3.5.3 Data distribution analysis We analyzed the distribution of percentage of Lymphocytes, Lactate dehydrogenase and High sensitivity C-reactive protein by creating histograms for the original and the new data. We noticed that all variables are strongly left skewed which is unfavorable for the model because more reliable predictions are made if the predictors and the target variable are normally distributed. Trying to make our model better, we applied square root transformation. Then we trained a new model on them (also using Ada Boost Classifier) and we tested it on the new data: We can see that the model generally improved (the accuracy is higher) and it perform slightly better. 5.3.5.4 General drawbacks In addition to the aforementioned bias, our models can still be accused of learning on very small data sets not exceeding even 1000 observations, which still affects the uncertainty related to the effectiveness of the presented proposals. Moreover, the most promising model, developed on an external data set, unfortunately did not show sufficiently high efficiency to be useful in medical applications. In addition, the models that have been proposed by us, unfortunately, are not explainable models, which makes them less desirable by doctors. The last, and perhaps least obvious, disadvantage is that the data used for prediction alone is unattainable for single entities, as blood tests can only be performed by highly qualified medical personnel. This aspect makes solutions based on these models incomprehensible to the average person, which significantly limits their usefulness. 5.3.5.5 Summary To sum up, the most desirable effect of our work turns out to be an explainable model with high-quality predictions, based on a large database. In addition, it should be based on easy-to-obtain information about a person’s health and be understandable to both ordinary people and physicians. In search of research that would help us explore this branch of machine learning, we managed to find an article [6], which provided us with both a comprehensive set of data, understandable to everyone, and a very rich information background. 5.3.6 Transparent Machine Learning While working in Machine Learning and creating Artificial Intelligence one can often encounter an issue called the Black Box problem. It occurs when a model is complex, unexplainable and not transparent. Explainability solves this problem by “unpacking the Black Box” which is essential in building trust in the model. That is why we want to create an explainable model which could be useful to every person which doesn’t need any additional medical information and easy to understand and requires only data that everyone has access to. To create the model we use random forest from (5-3-ranger?) package constructing a multitude of decision trees which are one of the most transparent and easy to explain models, even for people not familiar with machine learning concepts. We used explainers from (5-3-DALEX?), package for eXplainable Machine Learning, to create visualizations, that allow user to understand where the results come from and because of that, they are more transparent and clear. 5.3.7 Data The data set used in the application is the same one that was used in the article (5-3-mexican_et_al?) and it is an open source data published by General Directorate of Epidemiology in Mexico. It consists of 150,000 records from Mexican hospitals, of which over 50,000 are patients with confirmed coronavirus. The most important data for the project were information on chronic diseases, age and date of death. 5.3.8 Application In order to achieve our goal we created an easy application, in which one can choose his or hers chronic diseases, sex, age and it calculates the COVID-19 mortality risk for particular infected person. Additionally, in the bookmarks there are presented plots about the model. The first one is a Break Down plot which shows how the contributions attributed to individual explanatory variables change the mean models prediction. Despite printing out the risk it also enables its user an easy option to understand the outcome. Another one is Ceteris Paribus profile which examines the influence of an explanatory variable (which is Age in this case) by assuming that the values of all other variables do not change. This visualization is very useful for doctors to properly distinguish a higher risk groups. The last one is a SHAP plot, it calculates the importance of a feature by comparing what a model predicts with and without the feature. Mortality Ceteris Paribus profile Mortality SHAP profile 5.3.9 Conclusions Thanks to our research, people may have easier access to transparent and explainable model that estimates mortality risk in case of being infected by COVID-19. Our application allows people to see which diseases are contributing the most to the outcome, without the need of doctor’s examination, such as any blood properties and other information, that can only be gathered by a specialist are not necessary. Only by selecting diseases that one suffers from, reliable prediction can be obtained quickly and without leaving home. References Bello-Chavolla, O. Y., Bahena-López, J. P., Antonio-Villa, N. E., Vargas-Vázquez, A., González-Díaz, A., Márquez-Salinas, A., et al. (2020). Predicting Mortality Due to SARS-CoV-2: A Mechanistic Score Relating Obesity and Diabetes to COVID-19 Outcomes in Mexico. The Journal of Clinical Endocrinology &amp; Metabolism, 105(8), 2752–2761. https://doi.org/10.1210/clinem/dgaa346 Yan, L., Zhang, H.-T., Goncalves, J., Xiao, Y., Wang, M., Guo, Y., et al. (2020c). An interpretable mortality prediction model for COVID-19 patients. Nature Machine Intelligence, 2(5), 283--288. https://www.nature.com/articles/s42256-020-0180-7 "],["comparison-of-neural-networks-and-tree-based-models-in-the-clinical-prediction-of-the-course-of-covid-19-illness.html", "5.4 Comparison of neural networks and tree-based models in the clinical prediction of the course of COVID-19 illness", " 5.4 Comparison of neural networks and tree-based models in the clinical prediction of the course of COVID-19 illness Authors: Jakub Fołtyn, Kacper Grzymkowski, Konrad Komisarczyk 5.4.1 Abstract The COVID-19 pandemic overwhelmed medical staff around the world, showing that effective and explainable models are needed to help allocate limited resources to those in need. Many published models for predicting COVID-19 related ICU admission and mortality were tree-based models or neural network models. We compared the two architectures in effectiveness, explainability and reproducibility. The two architectures appear to be similar with regards to their effectiveness, but the neural network model had significant reproducibility issues and worse explainability. 5.4.2 Introduction In 2020 many papers presenting models aimed to predict the course of COVID-19 in patients emerged (Ma et al. 2020; Yan et al. 2020d; Zheng et al. 2020). Article we are referring to (Li et al. 2020) differed from the majority with the use of neural networks. In the past there were many attempts to compare effectiveness of artificial neural networks and tree-based models at various tasks using different types of data including tabular data (Ahmad et al. 2017; Arsad et al. 2013) and tasks connected to hospital patients outcomes (J. Wang et al. 2009). There were analyses showing superiority of forests over simple neural networks on small (up to 1000 records) tabular datasets (Klambauer et al. 2017). Data used by the referred article’s authors consisted of 1020 or 1106 observations depending on the researched problem. Authors note the importance of explainability of clinical models (Tonekaboni et al. 2019) due to the need of establishing clinician’s trust to successfully deploy a model. There are tools for explaining tree-based models (Biecek and Burzykowski 2021; Chen and Guestrin 2016) and some of methods are not available for neural networks. Tools to calculate SHAP values for forest models were developed (Lundberg et al. 2019), while general algorithm to compute them exactly in a reasonable complexity for deep learning models, only approximations can be made. Explaining neural networks with SHAP values is an important issue in the field (R. Wang et al. 2021). The referred article does not provide a source code for the replication of the results. Taking care about the reproducibility is considered a major problem in the academic society (WUoT 2020). Motivated by the preceding we aim to contribute to the work started by the authors of the referred article. At first, we try to replicate models. Then we propose different network architectures and XGBoost models. Finally, we compare the effectiveness of all the models at the prediction tasks and their explanations based on the SHAP values. 5.4.3 Methods Three neural network models and XGBoost model were trained for both ICU admission prediction and death prediction. The tested neural network architectures were: replication of the architecture proposed by (Li et al. 2020) (referred to as the Baseline model), modified version of that architecture using binary cross entropy as the loss function (Baseline (crossentropy) or Modified)), basic neural network model using two hidden layers of 32 neurons each with binary cross entropy as the loss function (Basic). Neural network models were created using Keras Python library (5-4-tensorflow-2015?). All models were trained and tested on data provided in the article (Li et al. 2020). Feature selection was performed as described in the article. Data was split in a 75:25 (train:test) ration. To reduce overfitting, an internal 0.2 validation set size was used. Effectiveness of those models were compared using receiver operating characteristic area under curve (ROC AUC) metric (Hanley 2014). ROC AUC values were calculated using the test data held out from training. Neural network models were trained 25 times each to compare stability. SHAP values were calculated for the xgboost models using the R treeshap library (Komisarczyk, Konrad and Maksymiuk, Szymon and Koźmiński, Paweł and Biecek, Przemysław 2020). Approximate SHAP values were approximated for neural network models using DeepExplainer from the Python SHAP library. Feature importance was calculated using mean SHAP values and compared. 5.4.4 Results ROC curve comparison between all ICU admission models is shown in the figure 5.5. XGBoost model was the best-performing model, based on the ROC AUC score. The best-performing neural network model was the Basic model (the neural network model with only 2 layers), with a ROC AUC score of 0.696. In the mortality prediction task, however, the Basic neural network model outperformed all other models, as shown in figure 5.6. It is worth noting that in both cases the Baseline model, which was the replication of the (Li et al. 2020) model had the lowest ROC AUC scored, and therefore is indicated to have performed the worst. Figure 5.5: ROC curves comparison for ICU admission prediction models. The dashed line indicates a random classifier (a model that classifies values in a random way). Each row in the legend contains model’s line color, name and ROC AUC score. Neural network model names explained: Baseline - a replication of the model from (Li et al. 2020) article, Basic - basic model using two hidden layers of 32 neurons each with binary cross entropy as the loss function, Modified - modified version of the article model using binary cross entropy as the loss function. Figure 5.6: ROC curves comparison for mortality prediction models. The dashed line indicates a random classifier (a model that classifies values in a random way). Each row in the legend contains model’s line color, model’s and ROC AUC score. Neural network model names explained: Baseline - a replication of the model from (Li et al. 2020) article, Basic - basic model using two hidden layers of 32 neurons each with binary cross entropy as the loss function, Modified - modified version of the article model using binary cross entropy as the loss function. To ensure the reliability of neural network models, each model was trained and tested 25 times. The resulting boxplots can be seen in the figure 5.7 for the ICU admission prediction data and in the figure 5.8 for the mortality data. The comparison score for both cases was ROC AUC score. As we can see in figure 5.7, the Baseline model trained on ICU admission data has proven to be especially unreliable, with AUC score ranging from 0.35 up to 0.7, while other 2 models outperformed it, with both better scores and lower variances. Figure 5.8 shows that models trained and tested on the mortality data had overall better ROC AUC scores than their ICU admission counterparts, but some outliers can be noticed for both Baseline and Modified, reaching as low as 0.5 ROC AUC score. Figure 5.7: each boxplot represents 25 independent model tests. Neural network model names explained: Baseline - a replication of the model from (Li et al. 2020) article, Basic - basic model using two hidden layers of 32 neurons each with binary cross entropy as the loss function, Modified - modified version of the article model using binary cross entropy as the loss function. Figure 5.8: each boxplot represents 25 independent model tests. Neural network model names explained: Baseline - a replication of the model from (Li et al. 2020) article, Basic - basic model using two hidden layers of 32 neurons each with binary cross entropy as the loss function, Modified - modified version of the article model using binary cross entropy as the loss function. Feature importance comparison for ICU admission task is shown in figure 5.9. In all of these models Procalcitionin was the most important feature (it is worth reminding, that our models were trained only on the top 5 features for ICU admission task, and top 6 features for mortality prediction task), while other features’ significance differ between models. What is more, Procalcitionin was much more impactful for the neural network model, with mean SHAP value nearly doubling the value of the 2nd most import feature. Figure 5.9: a) Boruta algorithm results from the article. Only the top 5 features are taken into consideration. b) feature importance barplot for xgboost model c) feature importance barplot for Basic neural network (model using two hidden layers of 32 neurons each with binary cross entropy as the loss function). The bars in b) and c) represent the mean SHAP values for each feature. Feature importance comparison for mortality prediction task is shown in figure 5.10. Age was the most important feature for all of these models. As in ICU admission prediction task, other features’ significance differ between models. Figure 5.10: a) Boruta algorithm results from the article. Only the top 6 features are taken into consideration. b) feature importance barplot for xgboost model c) feature importance barplot for Basic neural network (model using two hidden layers of 32 neurons each with binary cross entropy as the loss function). The bars in b) and c) represent the mean SHAP values for each feature. 5.4.5 Discussion The COVID-19 pandemic has demonstrated a need for quick development, testing, validation and deployment of machine learning models. However, this can’t come at the expense of reproducibility, as the replication crisis still poses a serious issue. Parts of the description provided in the reference article can be imprecise enough to be misunderstood or have more than one meaning. In the field of machine learning both data and source code are necessary components to reproduce results. This is even more important when proposing unusual model architectures, as those can lead to new discoveries in the field. However, when proposing such an architecture, a baseline model should be used to demonstrate that the proposed architecture can preform better. Finally, different external validation techniques should be used, such as a leave-one-hospital-out cross-validation or working on combined data from multiple sources. While our team focused primarily on comparing model architectures, other chapters in the WB Book examine those aspects of fair artificial intelligence. In conclusion, we have shown the importance of providing reproducible code, as well as having a baseline model to compare the results. 5.4.6 Source code Source code for all models and figures mentioned in the article, including used data is available at the Github repository https://github.com/konrad-komisarczyk/wb2021 in the proj2 subdirectory. References Ahmad, M. W., Mourshed, M., &amp; Rezgui, Y. (2017). Trees vs Neurons: Comparison between random forest and ANN for high-resolution prediction of building energy consumption. Energy and Buildings, 147, 77--89. https://doi.org/10.1016/j.enbuild.2017.04.038 Arsad, P. M., Buniyamin, N., &amp; Manan, J. A. (2013). Prediction of engineering students’ academic performance using Artificial Neural Network and Linear Regression: A comparison. ICEED. https://doi.org/10.1109/iceed.2013.6908300 Biecek, P., &amp; Burzykowski, T. (2021). Explanatory Model Analysis. Chapman; Hall/CRC, New York. https://pbiecek.github.io/ema/ Chen, T., &amp; Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. KDD. https://doi.org/https://doi.org/10.1145/2939672.2939785 Hanley, J. A. (2014). Receiver Operating Characteristic (ROC) Curves. Wiley StatsRef: Statistics Reference Online. https://doi.org/10.1002/9781118445112.stat05255 Klambauer, G., Unterthiner, T., Mayr, A., &amp; Hochreiter, S. (2017). Self-Normalizing Neural Networks. arXiv:1706.02515. https://arxiv.org/abs/1706.02515 Komisarczyk, Konrad and Maksymiuk, Szymon and Koźmiński, Paweł and Biecek, Przemysław. (2020). treeshap: Fast SHAP values computation for ensemble models. https://github.com/ModelOriented/treeshap Li, X., Ge, P., Zhu, J., Li, H., Graham, J., Singer, A., et al. (2020). Deep learning prediction of likelihood of ICU admission and mortality in COVID-19 patients using clinical variables. PeerJ, 8. https://peerj.com/articles/10337/ Lundberg, S. M., Erion, G. G., &amp; Lee, S.-I. (2019). Consistent Individualized Feature Attribution for Tree Ensembles. ICML Workshop. https://arxiv.org/abs/1802.03888 Ma, X., Ng, M., Xu, S., Xu, Z., Qiu, H., Liu, Y., et al. (2020). Development and validation of prognosis model of mortality risk in patients with COVID-19. Epidemiology and Infection, 148. http://doi.org/10.1017/S0950268820001727 Tonekaboni, S., Joshi, S., McCradden, M. D., &amp; Goldenberg, A. (2019). What Clinicians Want: Contextualizing Explainable Machine Learning for Clinical End Use. Machine Learning for Healthcare. http://proceedings.mlr.press/v106/tonekaboni19a.html Wang, J., Li, M., Hu, Y., &amp; Zhu, Y. (2009). Comparison of hospital charge prediction models for gastric cancer patients: neural network vs. decision tree models. BMC Health Services Research, 9(1). https://doi.org/10.1186/1472-6963-9-161 Wang, R., Wang, X., &amp; Inouye, D. I. (2021). Shapley Explanation Networks. ICLR. https://openreview.net/forum?id=vsU0efpivw WUoT. (2020). ML case studies: Reproducibility of scientific papers. https://mini-pw.github.io/2020L-WB-Book/reproducibility.html Yan, L., Zhang, H.-T., Goncalves, J., Xiao, Y., Wang, M., Guo, Y., et al. (2020d). An interpretable mortality prediction model for COVID-19 patients. Nature Machine Intelligence, 2(5), 283--288. https://www.nature.com/articles/s42256-020-0180-7 Zheng, Y., Zhu, Y., Ji, M., Wang, R., Liu, X., Zhang, M., et al. (2020). A Learning-Based Model to Evaluate Hospitalization Priority in COVID-19 Pandemics. Patterns, 1(9), 100173. https://doi.org/10.1016/j.patter.2020.100173 "],["rashomonml.html", "Chapter 6 RashomonML", " Chapter 6 RashomonML "],["topic.html", "6.1 Topic", " 6.1 Topic Authors: Adrian Stańdo, Maciej Pawlikowski, Mariusz Słapek (Warsaw University of Technology) 6.1.1 Abstract Technological advances were early adopted by healthcare with great benefits and developments. In many health-related realms machine learning is crucial such as: development of new medical procedures, the treatment of chronic diseases, the management of patient records and data. Explainable AI (XAI) gives invaluable tools in healthcare for understanding the models by humans. The aim of this article is to compare PDP profiles for a Rashomon set with a given metric. Following we use five different metrics of distance function and based on that we compare PDP curves. Additionally, the library in the Python language has been created, which automates this research. We have observed that the created models, which have similar scores, have different variable importances. In this paper these differences were measured and assessed to understand better the problem of predicting hospital mortality using data from MIMIC-III. 6.1.2 Literature review Rashomon is a intriguing Japanese movie in which four people witness an incident from different vantage points. When they come to testify in court, they all report the same facts, but their stories of what happened are very different. In machine learning Rashomon set is used to characterise problem in which many different models offer accurate results describing the same data. However, not every accurate model gives a right conclusion as described in (Breiman and others 2001): “If the model is a poor emulation of nature, the conclusion might be worng”. Herein authors also explain basics of Rashomon sets on example. Much more in depth and mathematical description is provided in (Semenova et al. 2019). Another important topic related to Rashomon sets is analysing the feature importance of the model. It was described in this article (Fisher et al. 2019b), where authors suggested to study the maximum and minimum of variable importance across all models included in the Rashomon set. This technique was called MCR (Model Class Reliance). Furthermore, (Dong and Rudin 2020) presented technique to visualise the “cloud” of variable importance for models in the set, which could help us understand the Rashomon set and choose the one which give the best interpretation. The last question stated in the article (Rudin et al. 2021) was about choosing model from the Rashomon set. It might be a difficult task, especially when we lack good exploration tools. (Das et al. 2019) created a system called BEAMS that allows to choose the most important features. Next, the program searches the hypothesis space in order to find model which fits best to given constraints. Since this system works only with linear regression classifiers, (Rudin et al. 2021) stated a question if it is possible to design a simmilar system which will search only models within the Rashomon set. 6.1.3 Results 6.1.3.1 Results of models search 6.1.4 Best models 6.1.4.1 Boxplots of abs_sum metric for the best models 6.1.4.2 Boxplots of abs_sum metric for each feature 6.1.4.3 PDP curve for albumin_std 6.1.4.4 PDP curve for atempc_min References Breiman, L., &amp; others. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199–231. Das, S., Cashman, D., Chang, R., &amp; Endert, A. (2019). BEAMES: Interactive multimodel steering, selection, and inspection for regression tasks. IEEE Computer Graphics and Applications, 39(5), 20–32. https://doi.org/10.1109/MCG.2019.2922592 Dong, J., &amp; Rudin, C. (2020). Exploring the cloud of variable importance for the set of all good models. Nature Machine Intelligence, 2(12), 810–824. Fisher, A., Rudin, C., &amp; Dominici, F. (2019b). All models are wrong, but many are useful: Learning a variable’s importance by studying an entire class of prediction models simultaneously. Journal of Machine Learning Research, 20(177), 1–81. Rudin, C., Chen, C., Chen, Z., Huang, H., Semenova, L., &amp; Zhong, C. (2021). Interpretable machine learning: Fundamental principles and 10 grand challenges. arXiv preprint arXiv:2103.11251. Semenova, L., Rudin, C., &amp; Parr, R. (2019). A study in rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning. arXiv preprint arXiv:1908.01755. "],["title-1.html", "6.2 Title", " 6.2 Title Authors: Jan Borowski, Konstanty Kraszewski, Krzysztof Wolny 6.2.1 Literature review In 1950 Japanese director, Akira Kurosawa, presented film Rashomon. Movie resolves around four witnesses, that describes the same crime in four different ways. This situation was called Rashomon effect after the name of the movie. In other words Rashomon effect is a situation when we have multiple different descriptions to the same event. This term is commonly used in multiple sciences like sociology, psychology or history. At the begging of the 21st century Rashomon effect was introduced to predictive modelling by Leo Breiman and his work ‘Statistical modeling: The Two Cultures’(Breiman and others 2001). In this article he named Rashomon effect situation, where there are many approximately-equally accurate models. Although these models have similar results, they can differ, when it comes to the way they managed to achieve it. Breiman called for closer examination of the Rashomon effect and conclusions that can be drawn from it. Recently, we can observe growing interest in Rashomon effect, although there is still a lot to be discovered. One of the articles, that bring closer the problem is ‘A study in Rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning’[6-0-rashomon-intro]. It provides several approaches for estimating the size of the Rashomon effect as well as the usefulness of the Rashomon curve in model selection. References Breiman, L., &amp; others. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199–231. "],["roshomon-sets-of-in-hospital-mortality-prediction-random-forest-models.html", "6.3 Roshomon sets of in-hospital mortality prediction random forest models", " 6.3 Roshomon sets of in-hospital mortality prediction random forest models Authors: Jeugeniusz Winiczenko, Mikolaj Malec, Patryk Wrona (Warsaw University of Techcnology) 6.3.1 Abstract The concept of Rashomon set is gaining more and more popularity in machine learning world. However, most efficient ways of building and analysing such sets are yet to be discovered. The main aim of this study was to develop several approaches to creating Rashomon sets, examining their characteristics and using them for further predictions. Performance of models was estimated using the area under the receiver operating characteristic (AUC) curve. For models from Rashomon sets analysis of features’ importance and PDP curves was also conducted. In this study, physiological time-series and medical histories from the Medical Information Mart for Intensive Care (MIMIC-III) database were used. Random forest models were trained for mortality prediction task on 2 datasets; the first containing only physiological time-series and the second containing both physiological time-series and medical histories. For 2 sets of trained models, corresponding to 2 datasets, several Rashomon sets were created using different thresholds. 6.3.2 Related work Rashomon sets are sets of models performing extraordinarily well on a given task. In machine learning, this term was used for the very first time by Leo Breiman in his paper issued in 2001 (Breiman and others 2001). Just as the task could be any, like in our case predicting patient’s mortality, the use of given features in order to explain vary among many highly accurate models. Moreover, Leo Breiman also described this situation as the Rashomon Effect and explained details using exemplary models. Until recently, the Rashomon sets have been rarely a subject of scientific research. In 2019 (Semenova et al. 2019) approached the issue creating mathematical and statistical definitions and notations regarding such sets of models. They described Rashomon sets as subspaces of the hypothesis space, that is subsets of models having comparable performance as the best model with respect to a loss function. In order to define well the problem, they introduced Rashomon ratio (fraction of models in rashomon set and all models from hypothesis space) and shattering coefficient - the maximum number of ways any n data points can be classified using functions from the hypothesis space. Another outstanding remark concerning Rashomon set was made when in 2019 (Fisher et al. 2019b) emphasized the analysis of features’ importance within Rashomon sets. The authors suggested Model Class Reliance - a new variable importance (VI) tool to study the the range of VI values across all highly accurate models - models included in rashomon sets. Later, (Rudin et al. 2021) provided basic rules for interpretable machine learning and identified 10 technical challenge areas in interpretable machine learning. They emphasized the troubleshooting and easiness of using glass-box models today as well as their advantage over black-box models due to their inscrutable nature. In this article, Challenge number 9 involves understanding, exploring, and measuring the Rashomon set. The authors address questions about how to characterize and visualize rashomon sets, and finally, how to pick the best model out of rashomon set. The Variable Importance Clouds, introduced in (Dong and Rudin 2020), are an excellent tool that one can use to address the above problems. Sush cloud maps every variable to its importance for every well-performing model. In our work, we choose and visualize the Rashomon sets built on a set of features as well as their subset. We address the problem of searching the most crucial predictive variables among those Rashomon sets and investigate the impact of choosing subsets of input features on the whole process of determining Rashomon sets and their characteristics. References Breiman, L., &amp; others. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199–231. Dong, J., &amp; Rudin, C. (2020). Exploring the cloud of variable importance for the set of all good models. Nature Machine Intelligence, 2(12), 810–824. Fisher, A., Rudin, C., &amp; Dominici, F. (2019b). All models are wrong, but many are useful: Learning a variable’s importance by studying an entire class of prediction models simultaneously. Journal of Machine Learning Research, 20(177), 1–81. Rudin, C., Chen, C., Chen, Z., Huang, H., Semenova, L., &amp; Zhong, C. (2021). Interpretable machine learning: Fundamental principles and 10 grand challenges. arXiv preprint arXiv:2103.11251. Semenova, L., Rudin, C., &amp; Parr, R. (2019). A study in rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning. arXiv preprint arXiv:1908.01755. "],["rashomon-on-mimic-draft.html", "6.4 Rashomon on MIMIC - draft", " 6.4 Rashomon on MIMIC - draft Authors: Degórski Karol, Fic Piotr, Kamiński Adrian (Warsaw University of Technology) 6.4.1 Abstract Rashomon effect occurs when there are many different machine learning models with similar predictions. Therefore choosing only one model out of them may have impact on the final results, so it should be done consciously, carefully and with the help of XAI methods. In our study, we performed an analysis of different XGBoost models using PCA and KMeans algorithms, so that we explained the factors that influenced on their final behavior. The task that we reproduced was an in-hospital mortality prediction conducted by (Tang et al. 2018). For building rashomon sets we used publicly available MIMIC-III dataset, which contains medical information. Our results suggest that XGB models from rashomon set may be grouped into clusters in the reduced parameter space. 6.4.2 Review of the literature The term Rashomon effect was created to describe a situation when there are many different models with quite similar predictions. Very often there are many different descriptions giving about the same minimum error rate, so that we cannot point one model as the best (Breiman and others 2001). As an example of this effect in reality they gave Linear Regression model and finding 5 from 30 best describing variables of a given problem. In this case there are approximately 140,000 such subsets. The authors explained that usually we choose the model which has best results on a test set, although there may be also different subsets of 5 variables that give very similar results. They also noticed that this effect occurs in different models, such as decision trees or neural networks. Furthermore, (Semenova et al. 2019) contributed to expand the study about Rashomon effect. They defined Rashomon set as a subset of models that have similar performance to the best model in terms of loss function. Moreover they introduced Rashomon ratio, that represents the fraction of models that fit our data equally well. Also they explained that Rashomon curve is a function of empirical risk versus the Rashomon ratio. They saw that there is a good generalization of the Rashomon curve’s elbow model when choosing between performance and simplicity of the model. They found out that interpretability of model is connected with Rashomon sets. Accordingly, when the Rashomon set is large there may exist simpler and higher performing model. Analysis of the Rashomon effect is still a new and open for developement field of the interpretable machine learning. Because of that, there are remaining challanges and problems, which are missing a state-of-the-art approach. Some of them are a proper measure of the Rashomon set, the best techniques of its visualization and optimal choice of the model from the Rashomon set (Rudin et al. 2021). One of useful tools for the mentioned tasks is a framework called Variable Importance Clouds (Dong and Rudin 2020), which can be used for studying the variable importance among Rashomon set. 6.4.3 Raw results 6.4.3.1 PCA dimension reduction in hyper-parameters space 6.4.3.2 Mean hyper-parameters values among clusters 6.4.3.3 Mean score values among clusters 6.4.3.4 The best models from clusters: variable importance 6.4.3.5 The best models from clusters: PD plots 6.4.3.6 The best models from clusters: ALE plots 6.4.3.7 Metrics change versus hyper-parameters over rashomon set References Breiman, L., &amp; others. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199–231. Dong, J., &amp; Rudin, C. (2020). Exploring the cloud of variable importance for the set of all good models. Nature Machine Intelligence, 2(12), 810–824. Rudin, C., Chen, C., Chen, Z., Huang, H., Semenova, L., &amp; Zhong, C. (2021). Interpretable machine learning: Fundamental principles and 10 grand challenges. arXiv preprint arXiv:2103.11251. Semenova, L., Rudin, C., &amp; Parr, R. (2019). A study in rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning. arXiv preprint arXiv:1908.01755. Tang, F., Xiao, C., Wang, F., &amp; Zhou, J. (2018). Predictive modeling in urgent care: A comparative study of machine learning approaches. Jamia Open, 1(1), 87–98. "],["roshomon-sets-on-death-prediction-xgb-models-using-mimic-iii-database.html", "6.5 Roshomon sets on death prediction XGB models using MIMIC-III database", " 6.5 Roshomon sets on death prediction XGB models using MIMIC-III database Authors: Ada Gąssowska, Elżbieta Jowik (Warsaw University of Techcnology) 6.5.1 An initial literature review As the Rashomon Effect is not a common concept, any references to the term in the literature are somewhat limited. The phenomenon is considered to occur when the same matter can be explained equally aptly in multitudinous ways. Hence the core of the name concept is the title of the Kurosawa’s movie from 1950 in which each character has different perspective on the same crime. In relation to Machine Learning the Rashomon Effect term was first used in (Breiman and others 2001) to introduce a class of problems where many differing, accurate models exist to describe the same data i.e. to describe the case where there exist many models that are non-identical but almost-equally-accurate for a given issue. Breiman emphasized that the observation of many different accurate models on specific datasets is a common phenomenon. However, from 2001 on the topic has rarely been discussed. While doing research on different machine learning models, data was quite often not taken into consideration at all. As stated in the recent article (Semenova et al. 2019) Rashomon Effect is directly linked to the topic of Explainable Machine Learning. According to the paper, large volume of data in the Rashomon set might imply the existence of multiple explainable model performing on the dataset equally accurately. The article aims to analyze the Rashomon effect on various datasets and attempt to formulate a statement regarding the information about the machine learning problem carried by the size of the Rashomon set. Another matter closely related to the Rashomon effect that needs to be addressed, is the variables importance analysis. This area of research is described in an (Fisher et al. 2019b) article. The publication emphasizes the existence the fields where Explainable Machine Learning (including Rashomon effect) is particularly important, as the non-explainable models may rely on undesirable variables. In an (Dong and Rudin 2020) paper, it is pointed out that only by comparing many models of similar performance the importance of a variable compared to other variables can be profoundly understood. The authors presented the concept of variable importance cloud and conducted the research showing that the variable importance may dramatically differ in approximately equally good models. Ahmad, M. W., Mourshed, M., &amp; Rezgui, Y. (2017). Trees vs Neurons: Comparison between random forest and ANN for high-resolution prediction of building energy consumption. Energy and Buildings, 147, 77--89. https://doi.org/10.1016/j.enbuild.2017.04.038 Andriawan, Z. A., Purnama, S. R., Darmawan, A. S., Ricko, Wibowo, A., Sugiharto, A., &amp; Wijayanto, F. (2020). Prediction of hotel booking cancellation using CRISP-DM. In 2020 4th international conference on informatics and computational sciences (ICICoS) (pp. 1–6). https://doi.org/10.1109/ICICoS51170.2020.9299011 Anthimopoulos, M., Christodoulidis, S., Ebner, L., Geiser, T., Christe, A., &amp; Mougiakakou, S. (2019). Semantic segmentation of pathological lung tissue with dilated fully convolutional networks. IEEE Journal of Biomedical and Health Informatics, 23(2), 714–722. https://doi.org/10.1109/JBHI.2018.2818620 Antonio, N., Almeida, A. de, &amp; Nunes, L. (2017). Predicting hotel booking cancellations to decrease uncertainty and increase revenue. Tourism &amp; Management Studies, 13(2), 25–39. https://doi.org/10.18089/tms.2017.13203 Antonio, N., Almeida, A. de, &amp; Nunes, L. (2019). An automated machine learning based decision support system to predict hotel booking cancellations. Data Science Journal, 18(1), 1–20. https://doi.org/10.5334/dsj-2019-032 Arsad, P. M., Buniyamin, N., &amp; Manan, J. A. (2013). Prediction of engineering students’ academic performance using Artificial Neural Network and Linear Regression: A comparison. ICEED. https://doi.org/10.1109/iceed.2013.6908300 Asadi-Aghbolaghi, M., Azad, R., Fathy, M., &amp; Escalera, S. (2020). Multi-level context gating of embedded collective knowledge for medical image segmentation. https://arxiv.org/abs/2003.05056 Azad, R., Asadi-Aghbolaghi, M., Fathy, M., &amp; Escalera, S. (2019). Bi-directional ConvLSTM u-net with densley connected convolutions. In Proceedings of the IEEE/CVF international conference on computer vision (ICCV) workshops. Baniecki, H., Kretowicz, W., Piatyszek, P., Wisniewski, J., &amp; Biecek, P. (2020). dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python. arXiv:2012.14406. https://arxiv.org/abs/2012.14406 Barish, M., Bolourani, S., Lau, L. F., Shah, S., &amp; Zanos, T. P. (2021). External validation demonstrates limited clinical utility of the interpretable mortality prediction model for patients with COVID-19. Nature Machine Intelligence, 3(1), 25–27. https://doi.org/10.1038/s42256-020-00254-2 Bello-Chavolla, O. Y., Bahena-López, J. P., Antonio-Villa, N. E., Vargas-Vázquez, A., González-Díaz, A., Márquez-Salinas, A., et al. (2020). Predicting Mortality Due to SARS-CoV-2: A Mechanistic Score Relating Obesity and Diabetes to COVID-19 Outcomes in Mexico. The Journal of Clinical Endocrinology &amp; Metabolism, 105(8), 2752–2761. https://doi.org/10.1210/clinem/dgaa346 Berk, R., Heidari, H., Jabbari, S., Kearns, M., &amp; Roth, A. (2017). Fairness in Criminal Justice Risk Assessments: The State of the Art. Sociological Methods &amp; Research. https://doi.org/10.1177/0049124118782533 Biecek, P., &amp; Burzykowski, T. (2021). Explanatory Model Analysis. Chapman; Hall/CRC, New York. https://pbiecek.github.io/ema/ Bird, S., Dudík, M., Edgar, R., Horn, B., Lutz, R., Milan, V., et al. (2020). Fairlearn: A toolkit for assessing and improving fairness in AI (No. MSR-TR-2020-32). Microsoft. https://www.microsoft.com/en-us/research/publication/fairlearn-a-toolkit-for-assessing-and-improving-fairness-in-ai/ Breiman, L., &amp; others. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199–231. Chen, T., &amp; Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. KDD. https://doi.org/https://doi.org/10.1145/2939672.2939785 Chollet, F. (2017). Deep learning with python. Manning. Chouldechova, A. (2016). Fair Prediction with Disparate Impact: A Study of Bias in Recidivism Prediction Instruments. Big Data, 5. https://doi.org/10.1089/big.2016.0047 Code of Federal Regulations. (1978). SECTION 4D, UNIFORM GUIDELINES ON EMPLOYEE SELECTION PROCEDURES (1978). https://www.govinfo.gov/content/pkg/CFR-2014-title29-vol4/xml/CFR-2014-title29-vol4-part1607.xml Corbett-Davies, S., Pierson, E., Feller, A., Goel, S., &amp; Huq, A. (2017). Algorithmic Decision Making and the Cost of Fairness. https://doi.org/10.1145/3097983.3098095 Das, S., Cashman, D., Chang, R., &amp; Endert, A. (2019). BEAMES: Interactive multimodel steering, selection, and inspection for regression tasks. IEEE Computer Graphics and Applications, 39(5), 20–32. https://doi.org/10.1109/MCG.2019.2922592 Dong, J., &amp; Rudin, C. (2020). Exploring the cloud of variable importance for the set of all good models. Nature Machine Intelligence, 2(12), 810–824. Dwork, C., Hardt, M., Pitassi, T., Reingold, O., &amp; Zemel, R. (2012). Fairness through awareness. ITCS. https://doi.org/10.1145/2090236.2090255 Falk, M., &amp; Vieru, M. (2018). Modelling the cancellation behaviour of hotel guests. International Journal of Contemporary Hospitality Management, 30(10), 3100–3116. https://doi.org/10.1108/ijchm-08-2017-0509 Fisher, A., Rudin, C., &amp; Dominici, F. (2019a). All Models are Wrong, but Many are Useful: Learning a Variable’s Importance by Studying an Entire Class of Prediction Models Simultaneously. Journal of Machine Learning Research, 20(177), 1–81. http://jmlr.org/papers/v20/18-760.html Fisher, A., Rudin, C., &amp; Dominici, F. (2019b). All models are wrong, but many are useful: Learning a variable’s importance by studying an entire class of prediction models simultaneously. Journal of Machine Learning Research, 20(177), 1–81. Géron, A. (2017). Hands-on machine learning with scikit-learn and TensorFlow : Concepts, tools, and techniques to build intelligent systems. O’Reilly Media. Hanley, J. A. (2014). Receiver Operating Characteristic (ROC) Curves. Wiley StatsRef: Statistics Reference Online. https://doi.org/10.1002/9781118445112.stat05255 Hardt, M., Price, E., Price, E., &amp; Srebro, N. (2016). Equality of Opportunity in Supervised Learning. NeurIPS. https://papers.nips.cc/paper/2016/hash/9d2682367c3935defcb1f9e247a97c0d-Abstract.html Karim, M. R., Döhmen, T., Rebholz-Schuhmann, D., Decker, S., Cochez, M., &amp; Beyan, O. (2020). DeepCOVIDExplainer: Explainable COVID-19 diagnosis from chest x-ray images. IEEE. Kaushal, A., Altman, R., &amp; Langlotz, C. (2020). Health Care AI Systems Are Biased. Scientific American. https://www.scientificamerican.com/article/health-care-ai-systems-are-biased Klambauer, G., Unterthiner, T., Mayr, A., &amp; Hochreiter, S. (2017). Self-Normalizing Neural Networks. arXiv:1706.02515. https://arxiv.org/abs/1706.02515 Komisarczyk, Konrad and Maksymiuk, Szymon and Koźmiński, Paweł and Biecek, Przemysław. (2020). treeshap: Fast SHAP values computation for ensemble models. https://github.com/ModelOriented/treeshap Li, X., Ge, P., Zhu, J., Li, H., Graham, J., Singer, A., et al. (2020). Deep learning prediction of likelihood of ICU admission and mortality in COVID-19 patients using clinical variables. PeerJ, 8. https://peerj.com/articles/10337/ Liu, C., Gao, C., Xia, X., Lo, D., Grundy, J., &amp; Yang, X. (2020). On the replicability and reproducibility of deep learning in software engineering. https://arxiv.org/abs/2006.14244 Lundberg, S. M., Erion, G. G., &amp; Lee, S.-I. (2019). Consistent Individualized Feature Attribution for Tree Ensembles. ICML Workshop. https://arxiv.org/abs/1802.03888 Ma, X., Ng, M., Xu, S., Xu, Z., Qiu, H., Liu, Y., et al. (2020). Development and validation of prognosis model of mortality risk in patients with COVID-19. Epidemiology and Infection, 148. http://doi.org/10.1017/S0950268820001727 Mendez, D., Graziotin, D., Wagner, S., &amp; Seibold, H. (2020). Open science in software engineering. Contemporary Empirical Methods in Software Engineering, 477–501. https://doi.org/10.1007/978-3-030-32489-6_17 Molnar, C. (2019). Interpretable Machine Learning: A Guide for Making Black Box Models Explainable. https://christophm.github.io/interpretable-ml-book Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., et al. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825–2830. Quanjel, M. J. R., Holten, T. C. van, Gunst-van der Vliet, P. C., Wielaard, J., Karakaya, B., Söhne, M., et al. (2021). Replication of a mortality prediction model in dutch patients with COVID-19. Nature Machine Intelligence, 3(1), 23–24. https://doi.org/10.1038/s42256-020-00253-3 R Core Team. (2018). R: A language and environment for statistical computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/ Riasi, A., Schwartz, Z., &amp; Chen, C.-C. (2019). A paradigm shift in revenue management? The new landscape of hotel cancellation policies. Journal of Revenue and Pricing Management, 18(6), 434–440. https://doi.org/10.1057/s41272-019-00189-3 Rudin, C., Chen, C., Chen, Z., Huang, H., Semenova, L., &amp; Zhong, C. (2021). Interpretable machine learning: Fundamental principles and 10 grand challenges. arXiv preprint arXiv:2103.11251. Sánchez-Medina, A. J., &amp; C-Sánchez, E. (2020). Using machine learning and big data for efficient forecasting of hotel booking cancellations. International Journal of Hospitality Management, 89, 102546. https://doi.org/10.1016/j.ijhm.2020.102546 Semenova, L., Rudin, C., &amp; Parr, R. (2019). A study in rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning. arXiv preprint arXiv:1908.01755. Smith, S. J., Parsa, H. G., Bujisic, M., &amp; Rest, J.-P. van der. (2015). Hotel cancelation policies, distributive and procedural fairness, and consumer patronage: A study of the lodging industry. Journal of Travel &amp; Tourism Marketing, 32, 886–906. https://doi.org/10.1080/10548408.2015.1063864 Tang, F., Xiao, C., Wang, F., &amp; Zhou, J. (2018). Predictive modeling in urgent care: A comparative study of machine learning approaches. Jamia Open, 1(1), 87–98. Tonekaboni, S., Joshi, S., McCradden, M. D., &amp; Goldenberg, A. (2019). What Clinicians Want: Contextualizing Explainable Machine Learning for Clinical End Use. Machine Learning for Healthcare. http://proceedings.mlr.press/v106/tonekaboni19a.html Wang, J., Li, M., Hu, Y., &amp; Zhu, Y. (2009). Comparison of hospital charge prediction models for gastric cancer patients: neural network vs. decision tree models. BMC Health Services Research, 9(1). https://doi.org/10.1186/1472-6963-9-161 Wang, L., Lin, Z. Q., &amp; Wong, A. (2020). COVID-net: A tailored deep convolutional neural network design for detection of COVID-19 cases from chest x-ray images. Scientific Reports, 10(1), 19549. https://doi.org/10.1038/s41598-020-76550-z Wang, R., Wang, X., &amp; Inouye, D. I. (2021). Shapley Explanation Networks. ICLR. https://openreview.net/forum?id=vsU0efpivw Wang, S., Zha, Y., Li, W., Wu, Q., Li, X., Niu, M., et al. (2020). A fully automatic deep learning system for COVID-19 diagnostic and prognostic analysis. European Respiratory Journal, 56(2). https://doi.org/10.1183/13993003.00775-2020 Wiens, J., Guttag, J., &amp; Horvitz, E. (2014). A study in transfer learning: leveraging data from multiple hospitals to enhance hospital-specific predictions. Journal of the American Medical Informatics Association, 21(4), 699–706. https://doi.org/10.1136/amiajnl-2013-002162 Wiśniewski, J., &amp; Biecek, P. (2021). fairmodels: A Flexible Tool For Bias Detection, Visualization, And Mitigation. arXiv:2104.00507. https://arxiv.org/abs/2104.00507 WUoT. (2020). ML case studies: Reproducibility of scientific papers. https://mini-pw.github.io/2020L-WB-Book/reproducibility.html Yan, L., Zhang, H.-T., Goncalves, J., Xiao, Y., Wang, M., Guo, Y., et al. (2020a). An interpretable mortality prediction model for COVID-19 patients. Nature Machine Intelligence, 2(5), 283–288. https://doi.org/10.1038/s42256-020-0180-7 Yan, L., Zhang, H.-T., Goncalves, J., Xiao, Y., Wang, M., Guo, Y., et al. (2020b). An interpretable mortality prediction model for COVID-19 patients. Nature Machine Intelligence, 2(5), 283–288. https://doi.org/10.1038/s42256-020-0180-7 Yan, L., Zhang, H.-T., Goncalves, J., Xiao, Y., Wang, M., Guo, Y., et al. (2020c). An interpretable mortality prediction model for COVID-19 patients. Nature Machine Intelligence, 2(5), 283--288. https://www.nature.com/articles/s42256-020-0180-7 Yan, L., Zhang, H.-T., Goncalves, J., Xiao, Y., Wang, M., Guo, Y., et al. (2020d). An interpretable mortality prediction model for COVID-19 patients. Nature Machine Intelligence, 2(5), 283--288. https://www.nature.com/articles/s42256-020-0180-7 Yildiz, B., Hung, H., Krijthe, J. H., Liem, C. C. S., Loog, M., Migut, G., et al. (2021). ReproducedPapers.org: Openly teaching and structuring machine learning reproducibility. In B. Kerautret, M. Colom, A. Krähenbühl, D. Lopresti, P. Monasse, &amp; H. Talbot (Eds.), Reproducible research in pattern recognition (pp. 3–11). Cham: Springer International Publishing. Zheng, Y., Zhu, Y., Ji, M., Wang, R., Liu, X., Zhang, M., et al. (2020). A Learning-Based Model to Evaluate Hospitalization Priority in COVID-19 Pandemics. Patterns, 1(9), 100173. https://doi.org/10.1016/j.patter.2020.100173 References Breiman, L., &amp; others. (2001). Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science, 16(3), 199–231. Dong, J., &amp; Rudin, C. (2020). Exploring the cloud of variable importance for the set of all good models. Nature Machine Intelligence, 2(12), 810–824. Fisher, A., Rudin, C., &amp; Dominici, F. (2019b). All models are wrong, but many are useful: Learning a variable’s importance by studying an entire class of prediction models simultaneously. Journal of Machine Learning Research, 20(177), 1–81. Semenova, L., Rudin, C., &amp; Parr, R. (2019). A study in rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning. arXiv preprint arXiv:1908.01755. "]]
